{"pages":[{"title":"about","text":"Hello :)","link":"/about/index.html"}],"posts":[{"title":"아파트 실거래가 분석 - 법정동 코드","text":"국토교통부는 공공데이터 포털에서 부동산 거래신고에 관한 법률에 따라 신고된 주택의 실거래 자료를 제공하고 있습니다. 실거래 자료를 받아오기 위해서는 법정동 코드를 알고 있어야 합니다. 본 글에선 법정동 코드에 대해 알아봅니다. 한국행정구역분류통계조사 시 지역자료의 분류와 집계, 지역 간 통계자료의 비교성 제고 등 목적을 가진 통계작성을 목적으로 전국의 행정구역을 일정한 순서에 따라 부호화한 자료로 대분류(시∙도), 중분류(시∙군∙구), 소분류(읍∙면∙동)의 3단계 위계구조의 총 7자리로 구성되는 특징이 있습니다. (대분류) 코드 7자리 중 앞 1,2번째 자리 서울(11), 6대광역시(2126), 세종(29), 9개도(3139) (중분류) 코드 7자리 중 앞 3,4,5번째 자리 시∙구(010290), 군(310990), 비자치구*(011,012~) (소분류) 코드 7자리 중 앞 6,7번째 자리 행정 읍∙면∙동과 인구와 토지가 있는 경우: 읍(1119), 면(3149), 동(51~99) 행정 읍∙면∙동은 없지만 인구와 토지가 있는 경우: (21~25) 행정 읍∙면∙동과 인구는 없지만 토지가 있는 경우: (26~29) 행정구역은 폐치분합과 승격, 명칭변경 수시로 변동되므로 분석하고자 하는 자료에 따라 적절한 코드 체계를 사용해야합니다. 출처 : 한국행정구역분류_2021.1.1.기준.xlsx 한국행정구역분류에서 관련 파일을 다운로드 받을 수 있습니다. 법정동 코드실거래가 수집을 위한 법정동 코드의 경우 한국행저욱역분류 파일에서도 확인가능하지만 행정표준코드관리시스템 또는 행정안전부 지방자치분권실 주민등록,인감,행정사에서도 확인할 수 있습니다. 법정동 코드는 한국행정구역분류 코드(7자리)와는 다르게 10자리로 구성되어 있으며 이는 법정동에 따라 다른 코드를 부여하기 때문입니다. 법정동 코드는 대분류(시∙도), 중분류(시∙군∙구), 소분류(법정동)의 3단계 위계구조의 총 10자리로 구성되는 특징이 있습니다. 본 글에서는 행정표준코드관리시스템에서 제공하는 법정동 코드 전체자료를 기반으로 가공 및 편집합니다. 데이터 읽어오기다운로드 받은 파일을 압축 해제하여 법정동코드 전체자료.txt를 DataFrame으로 읽어옵니다. 파일 읽기123import pandas as pdcode = pd.read_csv('법정동코드.txt', encoding='cp949', sep='\\t')code 폐지 코드 제외중복된 코드를 방지하기 위해 폐지된 코드는 제외하고 가지고 옵니다. 폐지 코드 제외12code = code[code['폐지여부'] == '존재']code 시군구 코드 추출단순히 실거래가 조회를 위한 시군구 코드를 추출하기 위해선 법정동코드의 앞 5자리만 추출하면 됩니다. 시군구 코드 추출123code['법정동코드'] = code['법정동코드'].astype('str')lawd_cd = code['법정동코드'].str[:5].drop_duplicates()lawd_cd 하지만 이경우 시도 코드가 포함되는 문제와 시군구명을 알기 어렵다는 단점이 존재합니다. 따라서 10자리 법정동 코드의 특징을 이용하여 3번째 자리가 0이 아니면서 6번째 자리가 0인 코드만 추출합니다. 시도 코드 제외(3번째 자리가 0), 읍면동 코드 제외(6번째 자리가 0이 아님) 시군구 코드 추출12lawd_cd = code.loc[(code['법정동코드'].str[2] != '0') &amp; (code['법정동코드'].str[5] == '0'), :]lawd_cd 시군구 코드 가공필요 없는 폐지여부 컬럼을 제거하고 법정동코드를 앞 5자리만 존재하도록 데이터를 가공합니다. 시군구 코드 추출123lawd_cd.drop('폐지여부', axis=1, inplace=True)lawd_cd['법정동코드'] = lawd_cd['법정동코드'].str[:5]lawd_cd 시도명과 시군구명을 위해 법정동명 컬럼을 공백을 기준으로 분할하여 첫번째 요소를 시도명으로 나머지 요소를 시군구명 컬럼으로 생성 후 시군구명 컬럼의 경우 join 함수를 이용하여 다시 하나의 문자열로 합칩니다. 시군구 코드 추출1234lawd_cd['시도명'] = lawd_cd['법정동명'].str.split().str[0]lawd_cd['시군구명'] = lawd_cd['법정동명'].str.split().str[1:]lawd_cd['시군구명'] = lawd_cd['시군구명'].apply(lambda x: &quot; &quot;.join(x))lawd_cd 시군구 코드 후 처리시군구 코드 가공을 마무리하면 다음과 같은 오류가 존재합니다. 세종특별자치시의 경우 시군구명이 없기 때문에 시군구명 컬럼에 빈 값이 존재 해당 컬럼에 세종특별자치시 추가 수원시, 성남시, 안양시, 안산시, 고양시, 용인시, 청주시, 천안시, 전주시, 포항시, 창원시도 포함됨 해당 시 제거 (해당 코드로 데이터를 요청해도 응답 결과가 없기 때문에 문제는 없으나 개발 계정의 경우 요청 건수가 1000건 제한으로 인해 낭비) 세종특별자치시 추가1lawd_cd.loc[lawd_cd['시군구명']=='', '시군구명'] = '세종특별자치시' 중복 시 제거123target = ['수원시', '성남시', '안양시', '안산시', '고양시', '용인시', '청주시', '천안시', '전주시', '포항시', '창원시']lawd_cd = lawd_cd.loc[~lawd_cd['시군구명'].isin(target)]lawd_cd 시군구 코드 저장법정동 코드1lawd_cd.to_excel('법정동코드.xlsx', index=False) 가공 완료된 법정동 코드 파일은 법정동코드(시군구)에서 확인 가능합니다.","link":"/2021/01/28/DATA-APT-02/"},{"title":"아파트 실거래가 분석 - 데이터 수집","text":"국토교통부는 공공데이터 포털에서 부동산 거래신고에 관한 법률에 따라 신고된 주택의 실거래 자료를 제공하고 있습니다. 실거래 자료를 받아오기 위해 Python을 이용하여 데이터를 수집 합니다. 만약 인증키가 없는 경우 인증키 발급이 필요합니다. 데이터 수집 (단일)코로나 감염 현황 분석 - 데이터 수집에서 사용한것과 동일하게 API 서버에 데이터 요청을 위한 requests, XML 형식의 응답 결과를 JSON 구조로 만들기 위한 xmltodict, 데이터프레임을 생성하고 데이터 가공에 특화된 Pandas 모듈을 사용합니다. 만약 해당 모듈이 설치가 되어있지 않으면 아래의 명령을 통해 모듈을 설치합니다. 모듈 설치lang:bash1pip install requests xmltodict pandas 필요 모듈 설치가 완료되면 아래의 코드를 실행하여 설치한 모듈을 import합니다. 모듈 설정lang:python12345# 필요한 모듈을 import 합니다.import json # Python에서 JSON을 사용하기 위한 모듈import requests # HTTP Requests를 위한 모듈import xmltodict # XML 형식을 JSON으로 변환하기 위한 모듈import pandas as pd # DataFrame을 생성하고 가공하기 위한 모듈 URL 작성HTTP Requests를 위해서는 URL을 작성해야합니다. 국토교통부 아파트 실거래가 자료의 경우 기본 서비스 URL은 http://openapi.molit.go.kr:8081/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade?_wadl&amp;type=xml 이며 서비스키(ServiceKey), 지역코드(LAWD_CD), 계약월(DEAL_YMD)을 필수로 포함해야합니다. 요청 변수(Request Parameter)| 항목명(영문) | 항목명(국문) | 항목 크기 | 항목 구분* | 샘플 데이터 | 항목설명 || —————- | —————- | —————– | ——————- | ——————- | ———————————————————— || LAWD_CD | 지역코드 | 5 | 1 | 11110 | 각 지역별 코드 행정표준코드관리시스템(www.code.go.kr)의 법정동코드 10자리 중 앞 5자리 || DEAL_YMD | 계약월 | 6 | 1 | 201512 | 실거래 자료의 계약년월(6자리) || serviceKey | 인증키 | 100 | 1 | 인증키 (URL Encode) | 공공데이터포털에서 발급받은 인증키 | 응답 결과(Response Element)| 항목명(영문) | 항목명(국문) | 항목설명 | 항목크기 | 항목구분 | 샘플데이터 || ———————- | —————- | ————– | ———— | ———— | ————————- || resultCode | 결과코드 | 결과코드 | 2 | 1 | 00 || resultMsg | 결과메세지 | 결과메세지 | 50 | 1 | NORMAL SERVICE. || Deal Amount | 거래금액 | 거래금액(만원) | 40 | 1 | 82,500 || Build Year | 건축년도 | 건축년도 | 4 | 1 | 2015 || Deal Year | 년 | 계약년도 | 4 | 1 | 2015 || Dong | 법정동 | 법정동 | 40 | 1 | 사직동 || Apartment Name | 아파트 | 아파트명 | 40 | 1 | 광화문풍림스페이스본(9-0) || Deal Month | 월 | 계약월 | 2 | 1 | 12 || Deal Day | 일 | 일 | 6 | 1 | 1 || Area for Exclusive Use | 전용면적 | 전용면적(㎡) | 20 | 1 | 94.51 || Jibun | 지번 | 지번 | 10 | 1 | 9 || Regional Code | 지역코드 | 지역코드 | 5 | 1 | 11110 || Floor | 층 | 층 | 4 | 1 | 11 | 따라서, http://openapi.molit.go.kr:8081/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade?_wadl&amp;type=xml?ServiceKey=발급받은서비스키&amp;LAWD_CD=지역코드&amp;DEAL_YMD=계약월와 같이 접속하면 데이터를 요청 할 수 있습니다. 데이터 요청요청 변수에 원하는 값을 입력 후 코드를 실행하면 정상적으로 데이터를 받아온 경우 &lt;Response [200]&gt;와 같은 결과가 나옵니다. 만약 200이 아닌 경우 요청 및 응답을 받아오는 과정에서 에러가 발생한 경우이며 [] 안에 있는 숫자가 에러 코드를 의미합니다. 데이터 요청12345678url = &quot;http://openapi.molit.go.kr:8081/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade?&quot;url += 'serviceKey=인증키'url += '&amp;LAWD_CD=법정동코드'url += '&amp;DEAL_YMD=거래년월'res = requests.get(url)res 응답 데이터를 보기 위해서는 응답 변수의 text 속성을 조회하면 확인 할 수 있습니다. 12# 응답 결과의 내용을 조회합니다.res.text 응답 데이터 변환응답 결과는 XML 형식으로 데이터가 기록되어 있고 원하는 데이터를 추출하기에 조금 어려움이 있습니다. 이를 쉽게 접근하기 위해 dict 형식으로 변환합니다. 1234567# 응답 결과의 내용을 Python Dict 형태로 변환합니다.res_json = xmltodict.parse(res.text) # 문자열을 XML을 JSON 형식으로 변환합니다.res_dict = json.dumps(res_json) # 변환된 JSON 형식을 Dictionary 문자열로 변환합니다.data = json.loads(res_dict) # Dictionary 문자열을 파이썬의 Dictionary 형식으로 변환합니다.data = json.loads(json.dumps(xmltodict.parse(res.text))) # 1줄로 작성된 변환 과정 사전 접근법을 이용하여 필요한 데이터에 접근합니다. 12# 필요한 데이터만 선택합니다.data['response']['body']['items'] 선택된 데이터만 데이터 프레임 형태로 변환합니다. 123# 선택된 데이터를 데이터 프레임으로 변환합니다.df = pd.DataFrame(data['response']['body']['items']['item'])df 변환된 파일을 파일로 저장합니다. 결과 저장1df.to_excel('아파트 실거래가_종로구_202101.xlsx', index=False) 데이터 수집 (반복)반복문을 사용하여 여러 시군구에 대해 데이터를 수집합니다. 본 예시는 서울특별시(25개)의 2018년 01월 ~ 2020년 12월 (36개월)의 데이터를 수집하는 코드입니다. 함수 정의 코드에서 API_KEY를 본인의 서비스키로 변경해야합니다. 함수 정의데이터 수집 함수 정의12345678910111213import json import requests import xmltodict import pandas as pd def get_data(lawd_cd, deal_ymd): base_url = &quot;http://openapi.molit.go.kr:8081/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTrade?serviceKey=API_KEY&quot; base_url += f'&amp;LAWD_CD={lawd_cd}' base_url += f'&amp;DEAL_YMD={deal_ymd}' res = requests.get(base_url) data = json.loads(json.dumps(xmltodict.parse(res.text))) df = pd.DataFrame(data['response']['body']['items']['item']) return df 함수 테스트1get_data(11680, 202101) 반복 일자 정의함수 테스트123seoul = [['강남구', '11680'], ['강동구', '11740'], ['강북구', '11305'], ['강서구', '11500'], ['관악구', '11620'], ['광진구', '11215'], ['구로구', '11530'], ['금천구', '11545'], ['노원구', '11350'], ['도봉구', '11320'], ['동대문구', '11230'], ['동작구', '11590'], ['마포구', '11440'], ['서대문구', '11410'], ['서초구', '11650'], ['성동구', '11200'], ['성북구', '11290'], ['송파구', '11710'], ['양천구', '11470'], ['영등포구', '11560'], ['용산구', '11170'], ['은평구', '11380'], ['종로구', '11110'], ['중구', '11140'], ['중랑구', '11260']]date = pd.date_range('20180101', '20201201', freq='MS').strftime('%Y%m') 데이터 수집 (반복)함수 테스트123456789apt = pd.DataFrame()for name, code in seoul: sgg = pd.DataFrame() for ym in date: temp = get_data(code, ym) sgg = pd.concat([sgg, temp]) sgg['시군구명'] = name apt = pd.concat([apt, sgg])apt 파일 저장함수 테스트1apt.to_csv('서울특별시 아파트 실거래가 201801~202012.csv', index=False)","link":"/2021/01/29/DATA-APT-03/"},{"title":"코로나 감염 현황 분석 - 인증키 발급","text":"보건복지부에서는코로나19 감염_현황를 신속 투명하게 공개하기 위해 오픈 API를 기반으로 발생 상황에 대한 정보를 제공하고 있습니다. 해당 오픈 API를 사용하기 위해서는 공공데이터 포털에서 인증키를 발급받아야 합니다. 인증키를 발급받기 위해서는 공공데이터 포털의 계정이 필요하므로 보유하고 있는 계정이 없는 경우 회원가입을 먼저 진행합니다. API 인증키 발급공공데이터 포털에서 로그인을 한 후 코로나 19 감염 현황을 검색하면 관련된 데이터의 목록이 조회가 됩니다. 스크롤을 아래로 내려 오픈 API 항목에 있는 보건복지부_코로나19 감염_현황을 클릭합니다. 상세 화면에서 활용 신청 버튼을 클릭합니다. 신청에 필요한 정보를 기재합니다. 발급받은 인증키의 경우 정상적으로 인증이 되기까지 최대 2시간의 시간이 필요할 수 있습니다. 또한 발급된 인증키의 경우 외부에 유출되지 않도록 관리가 필요합니다. 활용신청 상세 기능정보에서 제공하는 미리 보기 기능을 통해 다음과 같이 필요한 요청 변수와 해당 요청의 결과를 조회할 수 있습니다. 만약 SERVICE_KEY_IS_NOT_REGISTERED_ERROR와 같은 결과가 나오는 경우 최대 2시간의 시간이 필요할 수 있습니다.","link":"/2020/12/31/DATA-COVID-19-01/"},{"title":"아파트 실거래가 분석 - 데이터 가공","text":"","link":"/2021/01/30/DATA-APT-04/"},{"title":"아파트 실거래가 분석 - 데이터 시각화","text":"","link":"/2021/01/31/DATA-APT-05/"},{"title":"코로나 감염 현황 분석 - 데이터 수집","text":"코로나 감염 현황 정보를 받아오기 위해 Python을 이용하여 데이터를 수집 합니다. 만약 인증키가 없는 경우 인증키 발급이 필요합니다. Google Colaboratory본 과정은 Google Colaboratory 환경을 이용합니다. Google Colaboratory(이하, Colab)은 브라우저에서 Python Code를 작성하고 실행할 수 있는 개발 환경입니다. Colab은 무료로 CPU/GPU/TPU 환경을 사용 할 수 있으며 코드 공유가 쉽다는 장점을 가지고 있습니다. 브라우저에서 Google Colaboratory을 검색하거나 주소창에 https://colab.research.google.com 를 입력합니다. 하단의 새 노트를 클릭하면 Jupyter Notebook 환경과 유사한 Python Code를 작성하고 실행할 수 있는 개발 환경을 확인 할 수 있습니다. 좌측 상단의 제목인 Untitled0.ipynb을 클릭하면 파일명을 변경 할 수 있습니다. 데이터 수집모듈 설치데이터 수집을 위해선 서버에 데이터 요청을 해야합니다. Python 환경에서 HTTP Requests를 하기 위해 Requests 모듈과 XML 형식의 응답 결과를 JSON 구조로 만들기 위한 xmltodict 모듈과 데이터프레임을 생성하고 데이터 가공에 특화된 Pandas 모듈을 설치합니다. 1pip install requests xmltodict pandas 만약, Colab 환경을 사용하는 경우 xmltodict 모듈만 설치하면 됩니다. 필요 모듈 설치가 완료되면 아래의 코드를 실행하여 설치한 모듈을 import합니다. 12345# 필요한 모듈을 import 합니다.import json # Python에서 JSON을 사용하기 위한 모듈import requests # HTTP Requests를 위한 모듈import xmltodict # XML 형식을 JSON으로 변환하기 위한 모듈import pandas as pd # DataFrame을 생성하고 가공하기 위한 모듈 URL 작성HTTP Requests를 위해서는 URL을 작성해야합니다. 보건복지부 코로나 19 감염 현황의 경우 기본 서비스 URL은 http://openapi.data.go.kr/openapi/service/rest/Covid19/getCovid19InfStateJson 이며 서비스키(ServiceKey)를 필수로 포함해야합니다. 이외 필요 정보는 다음과 같습니다. 요청 변수(Request Parameter) 항목명(국문) 항목명(영문) 항목크기 항목구분 샘플데이터 항목설명 서비스키 ServiceKey 4 필수 - 공공데이터포털에서 받은 인증키 페이지 번호 pageNo 4 옵션 1 페이지번호 한 페이지 결과 수 numOfRows 4 옵션 10 한 페이지 결과 수 데이터 생성일 시작범위 startCreateDt 30 옵션 20200310 검색할 생성일 범위의 시작 데이터 생성일 종료범위 endCreateDt 30 옵션 20200315 검색할 생성일 범위의 종료 응답 결과(Response Element) 항목명(국문) 항목명(영문) 항목크기 항목구분 샘플데이터 항목설명 결과코드 resultCode 2 필수 00 결과코드 결과메시지 resultMsg 50 필수 OK 결과메시지 한 페이지 결과 수 numOfRows 4 필수 10 한 페이지 결과 수 페이지 번호 pageNo 4 필수 1 페이지번호 전체 결과 수 totalCount 4 필수 3 전체 결과 수 게시글번호(감염현황 고유값) SEQ 30 필수 74 게시글번호(감염현황 고유값) 기준일 STATE_DT 30 필수 20200315 기준일 기준시간 STATE_TIME 30 필수 00:00 기준시간 확진자 수 DECIDE_CNT 15 필수 8162 확진자 수 격리해제 수 CLEAR_CNT 15 필수 834 격리해제 수 검사진행 수 EXAM_CNT 15 필수 16272 검사진행 수 사망자 수 DEATH_CNT 15 필수 75 사망자 수 치료중 환자 수 CARE_CNT 15 필수 7253 치료중 환자 수 결과 음성 수 RESUTL_NEG_CNT 15 필수 243778 결과 음성 수 누적 검사 수 ACC_EXAM_CNT 15 필수 268212 누적 검사 수 누적 검사 완료 수 ACC_EXAM_COMP_CNT 15 필수 251940 누적 검사 완료 수 누적 환진률 ACC_DEF_RATE 30 필수 3.2396602365 누적 환진률 등록일시분초 CREATE_DT 30 필수 2020-03-15 10:01:22.000 등록일시분초 수정일시분초 UPDATE_DT 30 필수 null 수정일시분초 따라서, http://openapi.data.go.kr/openapi/service/rest/Covid19/getCovid19InfStateJson?ServiceKey=발급받은서비스키와 같이 접속하면 데이터를 요청 할 수 있습니다. 데이터 요청발급 받은 인증키를 API_KEY 부분에 입력 후 코드를 실행하면 정상적으로 데이터를 받아온 경우 &lt;Response [200]&gt;와 같은 결과가 나옵니다. 만약 200이 아닌 경우 요청 및 응답을 받아오는 과정에서 에러가 발생한 경우이며 [] 안에 있는 숫자가 에러 코드를 의미합니다. 12345678910# Requests를 위한 URL을 생성합니다.# Requests를 위한 URL을 생성합니다.url = 'http://openapi.data.go.kr/openapi/service/rest/Covid19/getCovid19InfStateJson?'url += 'serviceKey=API_KEY'url += '&amp;startCreateDt=20200101'url += '&amp;endCreateDt=20201231'res = requests.get(url)res 응답 데이터를 보기 위해서는 응답 변수의 text 속성을 조회하면 확인 할 수 있습니다. 12# 응답 결과의 내용을 조회합니다.res.text 응답 데이터 변환응답 결과는 XML 형식으로 데이터가 기록되어 있고 원하는 데이터를 추출하기에 조금 어려움이 있습니다. 이를 쉽게 접근하기 위해 dict 형식으로 변환합니다. 12345678# 응답 결과의 내용을 Python Dict 형태로 변환합니다.res_json = xmltodict.parse(res.text) # 문자열을 XML을 JSON 형식으로 변환합니다.res_dict = json.dumps(res_json) # 변환된 JSON 형식을 Dictionary 문자열로 변환합니다.data = json.loads(res_dict) # Dictionary 문자열을 파이썬의 Dictionary 형식으로 변환합니다.data = json.loads(json.dumps(xmltodict.parse(res.text))) # 1줄로 작성된 변환 과정 사전 접근법을 이용하여 필요한 데이터에 접근합니다. 12# 필요한 데이터만 선택합니다.data['response']['body']['items'] 선택된 데이터만 데이터 프레임 형태로 변환합니다. 123# 선택된 데이터를 데이터 프레임으로 변환합니다.df = pd.DataFrame(data['response']['body']['items']['item'])df 변환된 파일을 파일로 저장합니다. 결과 저장12# seq 컬럼의 값을 기준으로 데이터 프레임을 오름차순 정렬 후 파일로 저장합니다.df.sort_values('seq').to_excel('코로나 19 감염 현황.xlsx', index=False) Colab 환경의 경우 좌측에 있는 폴더 모양 아이콘을 클릭하여 파일로 저장된 코로나 19 감염 현황 데이터를 다운로드 받습니다. 다운로드 받은 파일을 열어보면 다음과 같은 결과를 확인 할 수 있습니다. 본 과정에서 사용한 코드는 아래의 링크를 클릭하면 확인 할 수 있습니다. [공공데이터 분석] 코로나 감염 현황 분석 - 데이터 수집","link":"/2021/01/01/DATA-COVID-19-02/"},{"title":"코로나 감염 현황 분석 - 데이터 가공","text":"오픈API를 이용해서 받아온 코로나 감염 현황 정보를 데이터를 가공합니다. 만약 데이터가 없는 경우 데이터 수집이 필요합니다.데이터만 필요한 경우 다운로드를 클릭하여 데이터를 다운로드 받을 수 있습니다. 데이터 업로드Google Colab 환경에서 외부 데이터를 읽어오는 방법은 업로드 하는 방법과 구글 드라이브를 마운트해서 읽어오는 방법이 있습니다. 본 글에서는 직접 업로드를 통해 외부 데이터를 Colab 환경에 추가합니다. 이를 위해 좌측의 폴더 모양 아이콘을 클릭합니다. 만약 세션에 연결되어있지 않다면 자동으로 세션을 연결 합니다. 이후 좌측의 업로드 아이콘을 클릭하여 업로드 하고자 하는 파일을 선택합니다. 업로드가 완료되면 데이터를 읽어 옵니다. 만약 에러가 발생하는 경우 파일을 선택 후 우클릭을 통해 파일 이름을 변경하고 다시 데이터를 읽으면 정상적으로 읽어옵니다. 데이터 읽어오기앞서 데이터 수집 과정 저장한 코로나 19 감염 현황.xlsx을 Pandas 모듈을 이용하여 DataFrame으로 불러옵니다. 1234import pandas as pddf = pd.read_excel('코로나 19 감염 현황.xlsx')df 컬럼명 정리현재 영어로 설정된 컬럼명을 한글로 지정하기 위해 참고 문서를 다운로드 받거나 오픈API 상세 페이지 출력결과(Response Element)를 참고하여 다음과 같이 컬럼명을 변경합니다. 컬럼명을 변경하는 방법은 columns 속성을 이용하는 방법과 rename 함수를 이용하는 방법이 있습니다. 컬럼명 전체를 바꾸고자 하는 경우에는 columns 속성을 특정 대상의 컬럼명만 바꾸고자 하는 경우에는 rename 함수를 추천합니다. columns 속성을 이용한 컬럼명 변경 (전체 변경)1234df.columns = ['누적 환진률', '누적 검사 수', '누적 검사 완료 수', '치료중 환자 수', '격리해제 수', '등록일시분초', '사망자 수', '확진자 수', '검사진행 수', '결과 음성 수', '게시글번호', '기준일', '기준시간', '수정일시분초']df rename 함수를 이용한 컬럼명 변경 (특정 대상 변경)123456789101112131415df.rename(columns={'accDefRate': '누적 환진률', 'accExamCnt': '누적 검사 수', 'accExamCompCnt': '누적 검사 완료 수', 'careCnt': '치료중 환자 수', 'clearCnt': '격리해제 수', 'createDt': '등록일시분초', 'deathCnt': '사망자 수', 'decideCnt': '확진자 수', 'examCnt': '검사진행 수', 'resutlNegCnt': '결과 음성 수', 'seq': '게시글번호', 'stateDt': '기준일', 'stateTime': '기준시간', 'updateDt': '수정일시분초'}, inplace=True)df 결측치 처리COVID-19 초기에는 누적 환진률, 누적 검사 수와 같은 데이터가 존재하지 않기 때문에 값이 NaN으로 저장되어 있습니다. NaN이 있는 경우 데이터 연산이 정상적으로 이뤄지지 않을 수 있습니다. 이를 위해 NaN의 값을 전부 0으로 변경합니다. 12df.fillna(0, inplace=True)df 데이터 재정렬데이터 수집 과정에서 ‘게시글번호’(‘seq’)를 기준 컬럼으로 데이터를 정렬 했습니다. 정렬 결과를 보면 1, 2, 3 순서가 아닌 1, 10, 100으로 정렬된것을 확인할 수 있습니다. 이는 데이터를 수집하고 저장하는 과정에서 해당 컬럼의 데이터 타입이 문자열이기 때문에 발생한 문제입니다. 데이터를 재 정렬 합니다. 12df.sort_values('게시글번호', inplace=True)df 불필요한 컬럼 제거등록일시분초, 수정일시분초, 기준시간, 게시글번호 컬럼은 데이터 분석 과정에서 불필요한 컬럼으로 제거 합니다. 제거 하는 방법은 drop 함수를 이용하거나 등록일시분초과 수정일시분초 컬럼을 제외한 나머지 컬럼을 선택하는 형태로 사용 할 수 있습니다. 12df.drop(['등록일시분초', '수정일시분초', '기준시간', '게시글번호'], axis=1, inplace=True)df 데이터 형 변환현재 날짜를 담당하는 기준일 컬럼은 숫자형으로 인식되었습니다. 주차별, 월별과 같은 정보를 추출하기 위해선 datetime 형식으로 변환이 필요합니다. Pandas의 to_datetime함수를 이용하여 날짜형으로 변환합니다. format 형식을 지정하는 경우 변환 속도가 빨라진다는 장점이 있습니다. 12df['기준일'] = pd.to_datetime(df['기준일'], format=&quot;%Y%m%d&quot;)df 중복 데이터 제거코로나19 감염 현황의 경우 추가적인 역학조사 결과 등에 따라 수정 및 보완될 수 있습니다. 만약 동일 날짜에 여러 데이터가 존재한다면 차후 신규 확진자 수 계산시 잘못 계산 될 수 있기 때문에 중복 데이터가 있는지 value_counts 함수를 통해 확인합니다. 1df['기준일'].value_counts() 2020-02-08 (5건), 2020-04-25, 2020-02-21, 2020-02-18 (3건) 등 과 같이 동일 날짜에 여러 데이터가 존재하는것을 확인 할 수 있습니다. 중복데이터의 값을 확인하고 싶은 경우 loc 인덱서를 통해서 해당 날짜의 데이터만 조회 할 수 있습니다. 1df.loc[df['기준일'] == '2020-02-08', :] 중복 값을 제거하고 가장 마지막에 수정된 값만 남기기 위해 drop_duplicates 함수를 이용합니다. 12df.drop_duplicates('기준일', keep='last', inplace=True)df 데이터 프레임 정리현재 컬럼은 데이터 수집 과정에서 추출된 태그명의 알파벳 순서대로 정렬되어 있습니다. 이를 보기 편하게 재설정하며 인덱스도 초기화 합니다. 123df = df[['기준일', '검사진행 수', '결과 음성 수', '확진자 수', '사망자 수', '격리해제 수', '치료중 환자 수', '누적 검사 수', '누적 검사 완료 수', '누적 환진률']].reset_index(drop=True)df 전일 대비 계산현재 확진자 수, 사망자 수와 같은 정보는 해당 날짜 기준 누적 데이터 입니다. 따라서 전일 대비를 계산하여 증감값을 조회합니다. 123new_columns = ['전일대비 ' + name for name in df.columns[3:7]]df.loc[:, new_columns] = df.iloc[:, 3:7].diff().valuesdf 날짜 정보 추출특정 날짜 정보에 따른 변화를 확인 하기 위해 datetime 형식으로 변환된 기준일 컬럼을 대상으로 datetimelike properties을 이용하여 주차별, 월별, 요일별과 같은 날짜 정보를 추출합니다. 123456df['기준일_연도'] = df['기준일'].dt.yeardf['기준일_월'] = df['기준일'].dt.monthdf['기준일_요일'] = df['기준일'].dt.day_name()df['기준일_월'] = df['기준일'].dt.weekdf['기준일_분기'] = df['기준일'].dt.quarterdf 데이터 저장최종 가공 완료된 데이터를 저장합니다. 1df.to_excel('코로나 19 감염 현황 가공 완료.xlsx') Colab 환경의 경우 좌측에 있는 폴더 모양 아이콘을 클릭하여 파일로 저장된 코로나 19 감염 현황 데이터를 다운로드 받습니다. 다운로드 받은 파일을 열어보면 다음과 같은 결과를 확인 할 수 있습니다. 본 과정에서 사용한 코드는 아래의 링크를 클릭하면 확인 할 수 있습니다. [공공데이터 분석] 코로나 감염 현황 분석 - 데이터 가공","link":"/2021/01/04/DATA-COVID-19-03/"},{"title":"코로나 감염 현황 분석 - 데이터 시각화","text":"오픈API를 이용해서 받아온 코로나 감염 현황 정보를 데이터를 시각화 합니다. 만약 데이터가 없는 경우 데이터 수집이 필요합니다.데이터만 필요한 경우 다운로드를 클릭하여 데이터를 다운로드 받을 수 있습니다.가공된 데이터를 만드는 과정은 데이터 가공에서 확인 할 수 있습니다. 데이터 업로드Google Colab 환경에서 외부 데이터를 읽어오는 방법은 업로드 하는 방법과 구글 드라이브를 마운트해서 읽어오는 방법이 있습니다. 본 글에서는 직접 업로드를 통해 외부 데이터를 Colab 환경에 추가합니다. 이를 위해 좌측의 폴더 모양 아이콘을 클릭합니다. 만약 세션에 연결되어있지 않다면 자동으로 세션을 연결 합니다. 이후 좌측의 업로드 아이콘을 클릭하여 업로드 하고자 하는 파일을 선택합니다. 글꼴 설정Colab에서 한글이 포함된 데이터를 시각화 하면 정상적으로 출력이 되지 않는 문제가 존재합니다. 이를 위해 한글 폰트를 설치하고 런타임을 재시작 합니다. 1234#실행 후 런타임 -&gt; 다시 시작!sudo apt-get install -y fonts-nanum!sudo fc-cache -fv!rm ~/.cache/matplotlib -rf 업로드가 완료되면 데이터를 읽어 옵니다. 만약 에러가 발생하는 경우 파일을 선택 후 우클릭을 통해 파일 이름을 변경하고 다시 데이터를 읽으면 정상적으로 읽어옵니다. 데이터 읽어오기분석에 필요한 모듈을 설정합니다. 12345import pandas as pd import matplotlib.pyplot as plt from matplotlib import rcParamsrcParams['axes.unicode_minus'] = Falseplt.rc('font', family='NanumBarunGothic') 앞서 데이터 가공 과정 저장한 코로나 19 감염 현황 가공 완료.xlsx을 Pandas 모듈을 이용하여 DataFrame으로 불러옵니다. 12df = pd.read_excel('코로나 19 감염 현황 가공 완료.xlsx')df 인덱스 설정Series 또는 DataFrame에서 plot 함수를 이용하여 데이터를 시각화 할 수 있습니다. index가 X축, value가 Y축으로 지정되며 DataFrame의 컬럼이 여러개인 경우 한 그래프에 각 컬럼별 차트가 추가됩니다. 이를 위해 기준일 컬럼을 인덱스로 지정합니다. 12df.set_index('기준일', inplace=True)df 일별 데이터 시각화DataFrame에서 원하는 컬럼만 시각화 하고 싶은 경우 데이터 프레임을 인덱싱 후 시각화 하거나 원하는 컬럼명을 지정 할 수 있습니다. 일별 확진자 수 시각화12df.plot(y='확진자 수', figsize=(10, 5))plt.show() 전일대비 확진자 수 시각화12df.plot(y='전일대비 확진자 수', figsize=(10, 5))plt.show() 신규 확진자 수 시각화123df['신규 확진자 수'] = df['전일대비 확진자 수'].abs().fillna(0)df.plot(y='신규 확진자 수', figsize=(10, 5))plt.show() 전일대비 누적 검사 완료 수 시각화123df['전일대비 누적 검사 완료 수'] = df['누적 검사 완료 수'].diff()df.plot(y='전일대비 누적 검사 완료 수', figsize=(10, 5))plt.show() 확진자 비율 시각화검사 수 대비 확진자 수 비율을 의미 123456789import datetimeimport numpy as npdf['확진자 비율'] = df['신규 확진자 수'] / df['전일대비 누적 검사 완료 수']value = df['확진자 비율'].fillna(0).valuesdf['확진자 비율'] = np.where(np.isinf(value), 0, value)plt.figure(figsize=(10, 5))plt.bar(df['확진자 비율'].index, df['확진자 비율'].values)plt.xlim([datetime.date(2020, 2, 1), datetime.date(2020, 12, 31)])plt.show() 월별 데이터 시각화기준일 재 수정123df['기준일_월'] = df.index.monthdf['기준일_주차'] = df.index.weekdf 월별 신규 확진자 수 합계 및 평균 계산12monthly = df.groupby(['기준일_월'])['신규 확진자 수'].agg(['sum', 'mean'])monthly 월별 신규 확진자 수 시각화12345678910fig, axis_y1 = plt.subplots()axis_y2 = axis_y1.twinx()axis_y2.plot(monthly['sum'].index, monthly['sum'].values, c='red')axis_y2.set_ylabel('신규 확진자 수 합계')axis_y1.bar(monthly['mean'].index, monthly['mean'].values)axis_y1.set_ylabel('신규 확진자 수 평균')axis_y1.set_xlabel('월 (2020년)')plt.xticks(range(1, 13))plt.title('월별 신규 확진자 수')plt.show() 주차별 데이터 시각화주차별 신규 확진자 수 합계 및 평균 계산12weekly = df.groupby(['기준일_주차'])['신규 확진자 수'].agg(['sum', 'mean'])weekly 주차별 신규 확진자 수 시각화12345678910fig, axis_y1 = plt.subplots()axis_y2 = axis_y1.twinx()axis_y2.plot(weekly['sum'].index, weekly['sum'].values, c='red')axis_y2.set_ylabel('신규 확진자 수 합계')axis_y1.bar(weekly['mean'].index, weekly['mean'].values)axis_y1.set_ylabel('신규 확진자 수 평균')axis_y1.set_xlabel('주차 (2020년)')plt.xticks(range(1, 13))plt.title('주차별 신규 확진자 수')plt.show()","link":"/2021/01/09/DATA-COVID-19-04/"},{"title":"아파트 실거래가 분석 - 인증키 발급","text":"국토교통부는 공공데이터 포털에서 부동산 거래신고에 관한 법률에 따라 신고된 주택의 실거래 자료를 제공하고 있습니다. 제공하는 API는 다음과 같습니다. 아파트매매 실거래자료 아파트 전월세 자료 연립다세대 매매 실거래자료 연립다세대 전월세 자료 단독/다가구 매매 실거래자료 단독/다가구 전월세 자료 오피스텔 매매 신고 조회 서비스 오피스텔 전월세 신고 조회 서비스 토지 매매 신고 조회 서비스 아파트 분양권전매 신고 자료 상업업무용 부동산 매매 신고 자료 해당 오픈 API를 사용하기 위해서는 공공데이터 포털에서 인증키를 발급받아야 합니다. 인증키를 발급받기 위해서는 공공데이터 포털의 계정이 필요하므로 보유하고 있는 계정이 없는 경우 회원가입을 먼저 진행합니다. API 인증키 발급공공데이터 포털에서 로그인을 한 후 실거래가를 검색하면 관련된 데이터의 목록이 조회됩니다. 스크롤을 아래로 내려 오픈 API 항목에 있는 국토교통부_아파트매매 실거래자료을 클릭합니다. 상세 화면에서 활용 신청 버튼을 클릭 후 신청에 필요한 정보를 기재합니다. 발급받은 인증키의 경우 정상적으로 인증이 되기까지 최대 2시간의 시간이 필요할 수 있습니다. 또한 발급된 인증키의 경우 외부에 유출되지 않도록 관리가 필요합니다. 활용신청 상세 기능정보에서 제공하는 미리 보기 기능을 통해 다음과 같이 필요한 요청 변수와 해당 요청의 결과를 조회할 수 있습니다. 만약 SERVICE_KEY_IS_NOT_REGISTERED_ERROR와 같은 결과가 나오는 경우 최대 2시간의 시간이 필요할 수 있습니다.","link":"/2021/01/27/DATA-APT-01/"},{"title":"코로나 영향 분석 - 데이터 수집","text":"서울 열린 데이터 광장에서는 제공하는 데이터를 이용하여 어느정도의 유동인구를 파악할 수 있습니다. 본 글에서는 생활 인구, 대중교통(버스, 지하철) 승하차 인원 정보 데이터 수집합니다. 데이터 다운로드다운로드 받은 파일은 다음과 같습니다. 행정동별 서울생활인구 - 내국인 행정동별 서울생활인구 - 단기체류 외국인 행정동별 서울생활인구 - 장기체류 외국인 행정구역 코드정보 서울시 버스노선별 정류장별 시간대별 승하차 인원 정보 서울시 지하철 호선별 역별 시간대별 승하차 인원 정보","link":"/2021/01/18/DATA-COVID-TRAFFIC-01/"},{"title":"데이터 분할하기","text":"우리는 머신러닝을 통해 현재의 데이터를 학습하여 미래의 데이터를 예측하고자 합니다. 이를 위해서는 데이터를 분할하여 올바르게 머신러닝 모델을 학습시켜야 합니다. 본 글에서는 데이터를 분할하는 방법에 대해 알아봅니다. 모델을 올바르게 학습시키기 위해서는 데이터를 분할해야 합니다. 모든 데이터를 학습에 사용한 경우 과대 적합이 발생하여 학습하지 않은 데이터의 성능이 매우 안 좋을 수 있고 현재 모델이 그런 문제를 가지고 있는지 알 수 없기 때문입니다. 즉 데이터의 일부를 학습에서 제외해서 학습이 완료된 모델을 통해 평가를 진행하여 현재 모델이 학습하지 않은 데이터에 대해 잘 예측할 수 있는지 확인하기 위해 사용됩니다. 데이터를 분할하는 방법은 크게 1) 학습(Train)과 평가(Test)로 분할하는 방법, 2) 학습(Train), 검증(Valid) 데이터와 평가(Test)로 분할하는 방법이 있습니다. 평가 데이터는 학습에 사용하지 않습니다. scikit-learn에서 데이터를 분할하기 위해서는 model_selection의 train_test_split 함수를 이용합니다. train_test_split에는 동일한 길이를 가지는(행의 수가 동일한) 여러 개의 변수를 동일하게 분할할 수 있습니다. 여러 개의 변수 모두 동일한 위치를 사용하여 각각 학습(Train)과 평가(Test)로 분할된 결과를 반환합니다. 분할을 위한 데이터 준비데이터 분할을 테스트 하기 위해 0부터 9까지 이루어진 array를 생성합니다. 12import numpy as npx = np.arange(10) 학습 및 평가 데이터 분할가장 많이 사용하는 방법은 학습 데이터와 평가 데이터로 데이터를 분할합니다. 일반적으로 가장 많이 사용하는 비율은 전체 데이터의 70%를 학습 데이터로 30%를 평가 데이터로 분할하여 학습 데이터만 머신 러닝 모델의 학습에 사용하는 형태입니다. scikit-learn에서는 기본적으로 75%:25% 비율로 분할합니다. 데이터 분할1개 변수 분할 (75%:25%)123from sklearn.model_selection import train_test_splittrain, test = train_test_split(x)train, test 데이터 분할 비율 변경학습 데이터와 평가 데이터의 비율은 train_size 또는 test_size를 통해 지정할 수 있습니다.test_size를 0.3으로 지정하는 경우 평가 데이터의 비율이 30%로, test_size가 0.5로 지정되는 경우 평가 데이터의 비율이 50%가 됩니다. 1개 변수 분할 (80%:20%)12train, test = train_test_split(x, test_size=0.2)train, test 데이터 분할 예시동일한 개수인 2개의 변수를 분할하는 경우 train_test_split 함수에 대입한 순서대로 분할됩니다. 2개 변수 분할123y = np.random.randint(0, 2, size=10)x_train, x_test, y_train, y_test = train_test_split(x, y)x_train, x_test, y_train, y_test 동일한 개수인 3개의 변수를 분할하는 경우 train_test_split 함수에 대입한 순서대로 분할됩니다. 3개 변수 분할123z = np.random.randn(10)x_train, x_test, y_train, y_test, z_train, z_test = train_test_split(x, y, z)x_train, x_test, y_train, y_test, z_train, z_test 3개 변수 분할123z = np.random.randn(10)z_train, z_test, y_train, y_test, x_train, x_test = train_test_split(z, y, x)z_train, z_test, y_train, y_test, x_train, x_test train_test_split함수에 대입된 변수가 순서대로 학습(train)과 평가(test)로 분할되고 해당 결과가 반환 됩니다. 학습 및 검증 및 평가 데이터 분할딥러닝에서는 학습 데이터와 평가 데이터로 데이터를 분할 후 학습 데이터를 한번 더 분할하여 학습 데이터와 검증 데이터로 나누어서 사용합니다. 이렇게 사용하는 이유는 학습하는 과정에서 모델이 정상적으로 학습이 되고 있는지 검증하기 위해서 사용됩니다. 평가 데이터를 검증 데이터로 지정하여 사용해도 학습에 관여하지 않기 때문에 상관은 없으나 만약 Early stopping을 사용하여 일찍 모델의 학습을 종료하는 경우 미래에 발생한 데이터의 성능이 가장 좋은 상황에서 모델의 학습이 멈추기 때문에 학습 데이터에서 일부를 분할하여 사용합니다. 일반적으로 가장 많이 사용하는 비율은 전체 데이터의 50%를 학습 데이터로 20%를 검증 데이터로 30%를 평가 데이터로 분할하여 학습 데이터만 머신 러닝 모델의 학습에 사용하는 형태입니다. 데이터 분할먼저 학습 데이터(70%)와 평가 데이터(30%)로 분할합니다. 2개 변수 분할12x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)x_train, x_test, y_train, y_test 이후 분할된 학습 데이터를 다시 학습 데이터(70%)와 검증 데이터(30%)로 분할합니다. 전체 10개의 데이터에서 7개가 학습, 3개가 평가용으로 분할 후 7개의 데이터를 다시 75%:25%으로 분할하면 5.25, 1.75개로 즉 5개 2개로 분할됩니다. 2개 변수 분할12x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.25)x_train, x_valid, y_train, y_valid train_test_split 함수의 파라미터(Parameter)train_test_split 함수의 경우 데이터를 셔플 후 분할합니다. 만약 데이터 셔플을 적용하지 않고 분할하는 경우 shuffle을 False로 지정합니다. test_size를 0.0에서 0.1 사이의 실수 값을 입력하여 평가 데이터의 비율을 지정할 수 있습니다. random_state를 지정하여 난수 발생기의 시드를 고정할 수 있습니다. 난수는 데이터를 셔플 할 때 사용하므로 시드가 고정되는 경우 동일한 분할 결과를 받을 수 있습니다. 재현성 확보를 위해 사용됩니다. stratify를 이용하여 층화 추출을 적용할 수 있습니다. 분류 모델의 경우 범주의 비율이 중요한데 학습 데이터와 평가 데이터의 범주 비율이 원본 데이터와 동일하게 분할하기 위해 사용합니다. 이외의 정보는 API를 클릭하면 확인할 수 있습니다.","link":"/2021/01/12/ML-SKLEARN-01/"},{"title":"코로나 영향 분석 - 데이터 처리","text":"서울 열린 데이터 광장에서는 제공하는 데이터를 이용하여 어느정도의 유동인구를 파악할 수 있습니다. 본 글에서는 생활 인구, 대중교통(버스, 지하철) 승하차 인원 정보 데이터를 하나의 데이터 프레임으로 결합 합니다. 행정동별 서울생활인구서울 열린 데이터 광장에서는 행정동별 서울생활인구는 압축파일 형태로 다운로드 받을 수 있습니다. 먼저 데이터 처리에 필요한 모듈을 설정합니다. 1234import osimport globimport zipfileimport pandas as pd 압축 해제를 위해 폴더 목록을 조회합니다. 폴더 목록 조회1234567base = os.path.join('data', '생활인구')folder = os.listdir(base)if '.DS_Store' in folder: folder.remove('.DS_Store')folder 각 폴더에 월별 압축 파일이 있는 구조입니다. data/생활인구/단기체류, data/생활인구/장기체류, data/생활인구/내국인 데이터 관리를 위해 압축파일이 있는 경로에 압축을 해제합니다. 각 폴더별 압축 해제1234for f in folder: zip_files = glob.glob(os.path.join(base, f, '*.zip')) for zip_file in zip_files: zipfile.ZipFile(zip_file).extractall(os.path.join(base, f)) 내국인glob 모듈을 이용하여 압축이 해제된 csv 파일을 조회하여 데이터를 읽어오고 병합합니다. 이때, 특정 파일의 경우 encoding이 cp949로 설정되어있습니다. UnicodeDecodeError가 발생합니다. try-except 구문을 이용하여 에러가 발생한 경우 encoding을 cp949로 설정합니다. 내국인 데이터 병합12345678910csv_files = glob.glob(os.path.join(base, '내국인', '*.csv'))df_local = pd.DataFrame()for csv_file in csv_files: try: temp = pd.read_csv(csv_file) except: temp = pd.read_csv(csv_file, encoding='cp949') df_local = pd.concat([df_local, temp])df_local 외국인단기체류 외국인과 장기체류 외국인 모두 구조가 동일합니다. 간혹 데이터의 구조가 다른 파일이 존재합니다. Excel을 이용하여 데이터를 읽은 경우 Numbers를 이용하여 데이터를 읽은 경우 Pandas를 이용하여 데이터를 읽은 경우 데이터 파일을 읽어오는 경우 shape이 일치하지 않는 경우가 존재하는데 이는 NA로 구성된 컬럼이 존재해서 그렇습니다. 이를 위해 인덱스로 지정된 값을 컬럼으로 변환하여 NA가 한개라도 있는 컬럼을 제거합니다. 이후 컬럼의 이름을 변경하여 데이터를 하나로 병합합니다. 단기체류glob 모듈을 이용하여 압축이 해제된 csv 파일을 조회하여 데이터를 읽어오고 병합합니다. 이때, 특정 파일의 경우 encoding이 cp949로 설정되어있습니다. try-except 구문을 이용하여 에러가 발생한 경우 encoding을 cp949로 설정합니다. 단기체류 외국인 데이터 병합123456789101112csv_files = glob.glob(os.path.join(base, '단기체류', '*.csv'))df_temp = pd.DataFrame()for csv_file in csv_files: try: temp = pd.read_csv(csv_file).reset_index().dropna(how='any', axis=1) except: temp = pd.read_csv(csv_file, encoding='cp949').dropna(how='any', axis=1) temp.columns = ['기준일ID', '시간대구분', '행정동코드', '총생활인구수', '중국인체류인구수', '중국외외국인체류인구수'] df_temp = pd.concat([df_temp, temp])df_temp 장기체류glob 모듈을 이용하여 압축이 해제된 csv 파일을 조회하여 데이터를 읽어오고 병합합니다. 이때, 특정 파일의 경우 encoding이 cp949로 설정되어있습니다. try-except 구문을 이용하여 에러가 발생한 경우 encoding을 cp949로 설정합니다. 장기체류 외국인 데이터 병합123456789101112csv_files = glob.glob(os.path.join(base, '장기체류', '*.csv'))df_long = pd.DataFrame()for csv_file in csv_files: try: temp = pd.read_csv(csv_file).reset_index().dropna(how='any', axis=1) except: temp = pd.read_csv(csv_file, encoding='cp949').dropna(how='any', axis=1) temp.columns = ['기준일ID', '시간대구분', '행정동코드', '총생활인구수', '중국인체류인구수', '중국외외국인체류인구수'] df_long = pd.concat([df_long, temp])df_long 파일 저장병합된 데이터 프레임을 파일로 저장합니다. 저장시 인덱스는 제외하고 저장합니다. 123df_temp.to_csv('서울생활인구_단기외국인(2019-2020).csv', index=False)df_long.to_csv('서울생활인구_장기외국인(2019-2020).csv', index=False)df_local.to_csv('서울생활인구_내국인(2019-2020).csv', index=False) 서울시 버스노선별 정류장별 시간대별 승하차 인원 정보서울시 버스노선별 정류장별 시간대별 승하차 인원 정보의 경우 월별 데이터 파일을 제공하며 2019년의 경우 상반기(01월~07월)은 한개의 파일로 존재합니다. 분할된 파일을 하나로 합치기 위해 데이터의 shape을 조회합니다. shape이 모두 동일하면 바로 데이터를 병합하면 됩니다. 데이터 조회1234csv_files = glob.glob(os.path.join('data', '대중교통', '버스', '*.csv'))for csv_file in csv_files: temp = pd.read_csv(csv_file, encoding='cp949') print(csv_file, temp.shape) 컬럼이 55개인 경우와 56개인 경우로 구분할 수 있는데 56개의 컬럼 구조는 55개의 컬럼 + 노선ID 입니다. 또한 2019년 8월 파일의 경우 버스정류장ARS번호 컬럼의 이름이 버스정류장번호로 되어있음을 유의합니다. 향후 분석에서 노선ID는 제외하고 분석하기 위해 shape이 56개인 경우 노선ID 컬럼을 제거하는 형태로 진행 후 병합합니다. 데이터 병합1234567891011121314151617181920212223csv_files = glob.glob(os.path.join('data', '대중교통', '버스', '*.csv'))bus = pd.DataFrame()for csv_file in csv_files: temp = pd.read_csv(csv_file, encoding='cp949') try: if temp.shape[-1] == 56: temp = temp.drop(['노선ID'], axis=1) temp.columns = ['사용년월', '노선번호', '노선명', '표준버스정류장ID', '버스정류장ARS번호', '역명', '00시승차총승객수', '00시하차총승객수', '1시승차총승객수', '1시하차총승객수', '2시승차총승객수', '2시하차총승객수', '3시승차총승객수', '3시하차총승객수', '4시승차총승객수', '4시하차총승객수', '5시승차총승객수', '5시하차총승객수', '6시승차총승객수', '6시하차총승객수', '7시승차총승객수', '7시하차총승객수', '8시승차총승객수', '8시하차총승객수', '9시승차총승객수', '9시하차총승객수', '10시승차총승객수', '10시하차총승객수', '11시승차총승객수', '11시하차총승객수', '12시승차총승객수', '12시하차총승객수', '13시승차총승객수', '13시하차총승객수', '14시승차총승객수', '14시하차총승객수', '15시승차총승객수', '15시하차총승객수', '16시승차총승객수', '16시하차총승객수', '17시승차총승객수', '17시하차총승객수', '18시승차총승객수', '18시하차총승객수', '19시승차총승객수', '19시하차총승객수', '20시승차총승객수', '20시하차총승객수', '21시승차총승객수', '21시하차총승객수', '22시승차총승객수', '22시하차총승객수', '23시승차총승객수', '23시하차총승객수', '등록일자'] except: display(temp) bus = pd.concat([bus, temp])bus.to_csv('서울시 버스노선별 정류장별 시간대별 승하차 인원 정보(2019-2020).csv', index=False) 서울시 지하철 호선별 역별 시간대별 승하차 인원 정보서울시 지하철 호선별 역별 시간대별 승하차 인원 정보의 경우 한개의 데이터 파일을 제공하며 2015년 01월부터 2020년 12월 데이터가 포함되어있습니다. 년도가 2020 또는 2019인 데이터들만 선택하여 저장합니다. 123subway = pd.read_csv('data/대중교통/지하철/서울시 지하철 호선별 역별 시간대별 승하차 인원 정보.csv', encoding='cp949')subway = subway.loc[subway['사용월'].astype('str').str[:4].isin(['2020', '2019']), :]subway.to_csv('서울시 지하철 호선별 역별 시간대별 승하차 인원 정보(2019-2020).csv', index=False)","link":"/2021/01/20/DATA-COVID-TRAFFIC-02/"},{"title":"데이터 불러오기","text":"scikit-learn에는 다양한 연습용 데이터 세트가 존재하며 함수를 통해 불러올 수 있습니다. 본 글에서는 데이터를 불러오는 방법에 대해 알아봅니다. 라이브러리의 구성은 크게 지도 학습, 비지도 학습, 모델 선택 및 평가, 데이터 변환으로 나눌 수 있으며 라이브러리를 사용하기 위해선 import 해야 합니다. 라이브러리를 통째로 불러오는 Numpy, Pandas와는 다르게 필요한 함수만 부르는 형태로 사용을 합니다. Scikit-Learn에서 사용되는 형식은 다음과 같습니다. 1from sklearn.위치 import 이름 DataSetsscikit-learn의 datasets API를 이용하면 다양한 연습용 데이터 세트를 사용 할 수 있습니다. 가상 데이터 생성 - Numpy Array make_blobs() : 군집을 위한 데이터 생성 make_moons() : 초승달 모양의 군집 데이터 생성 make_classification() : 분류를 위한 데이터 생성 make_regression() : 회귀를 위한 데이터 생성 연습용 데이터 세트 불러오기(소규모) - Bunch load_breast_cancer() : 위스콘신 유방암 데이터, 이진 분류 load_diabetes() : 당뇨병 환자 데이터, 회귀 load_digits() : 0~9 손글씨 데이터, 다중 분류 load_iris() : 붓꽃 품종 데이터, 다중 분류 load_linnerud() : 피트니스 클럽 데이터, 회귀 연습용 데이터 세트 불러오기(중/대규모) - Bunch fetch_mldata() : mldata.org에 존재하는 데이터 다운로드 가능 fetch_openml() : openml.org에 존재하는 데이터 다운로드 가능 fetch_20newsgroups() : 20개의 뉴스 그룹 데이터 fetch_rcv1() : 로이터 뉴스 말뭉치 scikit-learn의 Datasets에서 make로 시작하는 함수는 numpy ndarray 타입의 X(Feature)와 y(label)를 반환하며 load 또는 fetch로 시작하는 함수는 Bunch 객체를 반환합니다. Bunch 객체는 파이썬의 딕셔너리와 유사하며 keys()를 통해 키 목록 조회 가능합니다. Bunch 객체에서 공통적으로 사용되는 키는 다음과 같습니다. data : X 데이터(2차원 Numpy ndarray) target : Y 데이터(1차원 Numpy ndrray) feature_names : X 데이터에 대한 정보(컬럼명) target_names : Y 데이터에 대한 정보(분류 데이터에만 존재) DESCR : 데이터 설명문(문자열) DataSets 불러오기iris 데이터는 꽃받침(Sepal)과 꽃잎(Petal)의 길이(Length)와 넓이(Width)를 이용하여 붓꽃의 품종을 예측하기 위한 데이터 세트입니다. 분류(Classification) 또는 군집(Clustering) 모델에서 사용할 수 있습니다. http://suruchifialoke.com/2016-10-13-machine-learning-tutorial-iris-classification/ 123from sklearn.datasets import load_irisiris = load_iris()type(iris) Bunch 객체는 파이썬의 딕셔너리와 유사하기 때문에 keys 함수를 이용하여 접근 가능한 Key 목록을 조회할 수 있습니다. 1iris.keys() 해당 Key에 대한 Value를 조회하고 싶은 경우 사전 표기법을 이용하여 데이터를 조회할 수 있습니다. ‘data’는 학습에 사용되는 데이터이며 통계학에서는 독립 변수, 설명 변수라고 부르고 머신 러닝에서는 특성이라고 부릅니다. 1iris['data'] 1iris.data ‘target’는 정답 데이터로 사용되는 데이터이며 통계학에서는 종속 변수, 반응 변수라고 부르고 머신 러닝에서는 레이블이라고 부릅니다. 분류 문제는 범주형 레이블을 예측하고 회귀 문제는 연속형 레이블을 예측합니다. 즉, target 값이 범주형인 경우 분류 모델을 연속형인 경우 회귀 모델을 사용합니다. 1iris['target'] 1iris.target ‘data’와 ‘target’만 있어도 머신 러닝 모델을 학습하고 평가할 수 있습니다. 만약 Bunch 객체가 아닌 X(Feature)와 y(label)만 필요한 경우 return_X_y=True를 사용할 수 있습니다. 1data, target = load_iris(return_X_y=True) 하지만 데이터를 더 잘 이해하기 위해서 ‘feature_names’ 또는 ‘target_names’, ‘DESCR’과 같은 키를 이용하여 부가적인 정보를 조회할 수 있습니다.","link":"/2021/01/10/ML-SKLEARN-00/"},{"title":"데이터 전처리 - Dummy","text":"머신러닝 모델을 학습하기 위해서는 데이터가 숫자형으로 존재해야 합니다. 성별(‘Male’, ‘Female’), 지역(‘KR’, ‘US’)과 같은 범주형 특성을 사용하기 위해서 올바른 숫자 형태로 표현을 위한 데이터 전처리 과정이 필요합니다. 본 글에서는 Dummy 변환을 적용하는 방법에 대해 알아봅니다. Dummy (One-Hot)디지털 회로에서 원핫(One-Hot)은 하나의 하이(1) 비트와 다른 모든 로우(0) 비트 값의 법적 조합만 있는 비트 그룹입니다. 하나의 로우(0) 비트를 제외한 모든 비트가 하이(1)인 비트 그룹을 원 콜드(One-Cold)라고 부르기도 합니다. One-Hot은 머신러닝과 통계에서 범주형 데이터를 처리하는 데 자주 사용되는 방법입니다. 많은 머신러닝 모델은 입력 변수가 숫자여야 하므로 범주형 변수는 데이터 전처리 과정이 필요합니다. Label Encoding범주형 데이터는 명목형 또는 순서형으로 구분할 수 있습니다. 순서형 데이터는 그 값에 대한 순위를 가지므로 Label Encoding을 통해 숫자형 데이터로 변환할 수 있습니다. 또한 개별 값 사이에는 양적 관계가 없는 경우 순서 인코딩을 사용하면 잠재적으로 데이터에 가상의 순서 관계를 만들 수 있습니다. 순서형 데이터의 예로는 A부터 F까지의 시험 등급이 있으며, 6부터 1까지의 숫자를 사용하여 순위를 매길 수 있습니다. Label Encoding을 적용하기 위해서는 preprocessing의 LabelEncoder를 이용합니다. 적용전 적용후 KR 3 EU 4 US 1 JP 2 CN 0 LabelEncoder는 알파벳 순서대로 숫자를 부여합니다. LabelEncoder12from sklearn.preprocessing import LabelEncoderLabelEncoder().fit_transform([&quot;KR&quot;, &quot;EU&quot;, &quot;US&quot;, &quot;JP&quot;, &quot;CN&quot;]) 만약 머신러닝 모델을 학습할 때 LabelEncoder를 이용하는 경우 주의할 점이 있습니다. 바로 순서 관계가 존재하기 때문에 중국(CN, 0) &lt; 미국 (US, 1) &lt; 일본 (JP, 2) &lt; 한국 (KR, 3) &lt; 유럽 연합(EU, 4)과 같은 대소 관계가 생기게 됩니다. 또는 미국(US, 1) + 일본(JP, 2) = 한국(KR, 3)과 라는 결과가 만들어질 수도 있습니다.따라서 일반적인 경우에는 One-Hot Encoding을 많이 사용합니다. One-Hot EncodingOne-Hot Encoding을 preprocessing의 OneHotEncoder를 이용합니다. Country Country_CN Country_US Country_JP Country_KR Country_EU KR 0 0 0 1 0 EU 0 0 0 0 1 US 0 1 0 0 0 JP 0 0 1 0 0 CN 1 0 0 0 0 One-Hot Encoding은 기존 열의 고윳값을 가지는 새로운 열을 생성하여 해당 행의 값에 맞는 컬럼에 1(Hot, True)을 나머지 컬럼에는 0(Cold, False)을 추가하는 방법입니다. OneHotEncoder12from sklearn.preprocessing import OneHotEncoderOneHotEncoder(). fit_transform([[&quot;KR&quot;], [&quot;EU&quot;], [&quot;US&quot;], [&quot;JP&quot;], [&quot;CN&quot;]]).toarray() 또한, Pandas의 get_dummies 함수를 이용하여 변환할 수 있습니다. (권장) OneHotEncoder123import pandas as pddf = pd.DataFrame({'Country' : [&quot;KR&quot;, &quot;EU&quot;, &quot;US&quot;, &quot;JP&quot;, &quot;CN&quot;]})pd.get_dummies(df)","link":"/2021/01/19/ML-SKLEARN-03/"},{"title":"Series &amp; DataFrame Basic","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 Series와 DataFrame 객체를 이용하여 데이터를 분석 및 조작할 수 있습니다. 본 글에서는 Series와 DataFrame에 대한 기본 사용 방법에 대해 알아봅니다. 데이터 접근 방법파이썬에서는 기본적으로 이름 뒤에 []가 온다면 데이터를 선택한다는 의미를 가지고 있습니다. 이러한 형식은 Python str, list, tuple, dict Numpy Array와 같은 데이터 형식에서도 동일한 의미를 가집니다. 데이터를 선택하는 과정을 인덱싱(Indexing)이라고 하며 만약 가지고 올 값이 여러 개인 경우 슬라이싱(Slicing)을 이용할 수 있습니다. Sequence 자료형의 경우 위치(숫자)를 dict 자료형의 경우 이름(문자열)을 이용합니다. SeriesSeries의 경우 Python list, tuple, Numpy Array에서 사용하는것과 동일하게 대괄호([행 인덱스])를 통해 개별 데이터에 접근할 수 있습니다. 1234import pandas as pdseries = pd.Series([1, 2, 3, 4, 5])series[0], series[2], series[1:2] Series의 경우 문자열 인덱스를 가질 수 있기 때문에 이름(문자열)을 통해서도 개별 데이터에 접근할 수 있습니다. 123series = pd.Series([1, 2, 3], index=['a', 'b', 'c'])series['a'], series['a':'c'], series[2], series[-1] 이때 위치(숫자)도 사용 가능하며 Slicing의 경우 위치는 콜론 뒤에 데이터를 포함하지 않지만 문자열의 경우 포함한다는 점이 차이점입니다. DataFrameDataFrame도 동일하게 인덱싱을 이용할 수 있지만 차이점이 존재합니다. Numpy Array의 경우 [행 인덱스, 열 인덱스]로 2차원 배열에서 데이터를 선택 가능하지만 DataFrame에서는 해당 방법을 사용 시 에러가 발생할 수 있습니다. 물론 loc, iloc, iat, at과 같은 인덱서를 이용하면 Numpy Array와 동일(유사)하게 동작합니다. DataFrame은 대괄호를 2개를 사용하며 [열 인덱스][행 인덱스]를 통해 개별 값에 접근할 수 있습니다. DataFrame은 여러 개의 시리즈가 칼럼 명을 Key로 가지는 Dict 형식이라고 생각해볼 수 있습니다. 즉 첫 번째 대괄호를 사용하여 1개의 Series를 선택하고 선택된 Series를 기반으로 다시 행을 접근한다고 생각하면 됩니다. 1234dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=['A','B','C'])dataframe['A'] 만약 여러 개 또는 특정 범위의 컬럼을 선택하기 위해서 인덱싱을 한다면 생각한 것과 다른 결과가 나오거나 아예 데이터가 나오지 않을 수 있습니다. 이러한 이유는 대괄호 안에 슬라이싱을 적용할 때는 행 인덱스를 사용하기 때문입니다. 즉, 아래의 결과가 아무런 결과도 나오지 않는 이유는 행 인덱스는 [0, 1, 2]인데 ‘A’부터 ‘C’까지 데이터를 선택하라고 했기 때문에 결과가 나오지 않는 것을 확인할 수 있습니다. 1234dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=['A','B','C'])dataframe['A':'C'] 1234dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=['A','B','C'])dataframe[:2] 이러한 문제로 인해서 Pandas에서 데이터를 선택하는 과정은 loc, iloc, at, iat과 같은 인덱서를 이용하는 것을 추천합니다. 인덱서 바로가기 공통 기본 속성Series와 DataFrame 모두 index, values, columns(DataFrame만 존재), dtypes, shape, T, loc, iloc, at, iat과 같은 속성이 존재합니다. valuesvalues 속성은 Series 또는 DataFrame의 값을 Numpy Array 형식으로 반환합니다. 12series = pd.Series([1, 2, 3, 4, 5])series.values indexindex 속성은 Series 또는 DataFrame의 행 인덱스를 반환합니다. 추후 특정한 행 데이터 또는 조건에 성립하는 행 데이터를 삭제하기 위해선 인덱스 정보가 필요한데 이때 index 속성을 이용합니다. 12series = pd.Series([1, 2, 3, 4, 5])series.index columnscolumns 속성은 DataFrame에서만 사용할 수 있으며 열 인덱스(컬럼명)을 반환합니다. 1234dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=['A','B','C'])dataframe.columns 앞서 이야기 한 3가지(index, columns, values) 속성이 중요한 이유는 데이터를 조회하는 기능과 동시에 수정도 지원하기 때문입니다. 즉, 컬럼명을 변경하고 싶은 경우 columns 속성에 새로운 컬럼명 리스트(동일한 데이터 개수를 가진)를 대입하는 경우 컬럼명이 변경됩니다. index 도 동일하며 index의 경우 행 인덱스를 변경됩니다. 123456dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=['A','B','C'])dataframe.columns = ['X1', 'X2', 'X3']dataframe.index = ['a', 'b', 'c']dataframe dtypes데이터 유형을 조회할 수 있는 속성으로 DataFrame의 경우 각 열 별 데이터 타입의 결과가 반환됩니다. series의 경우 dtype 속성으로도 조회 가능합니다. (데이터 프레임은 불가) 1234dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=['A','B','C'])dataframe.dtypes shape데이터의 형태를 조회할 수 있는 속성으로 Series의 경우 행 데이터 수가, DataFrame의 경우 행, 열 데이터 수가 튜플 형식으로 출력됩니다. 1234dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=['A','B','C'])dataframe.shape # (행의 수, 열의 수) TTranspose의 줄임말이며 행과 열이 변경된 결과를 반환합니다. 1234dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=['A','B','C'])dataframe.T loc, iloc, at, iat데이터에 접근할때 사용하는 인덱서(Indexer)로 Numpy Array에서 사용한 Index과 Slicing을 동일하게 진행 할 수 있습니다. 해당 부분은 데이터를 선택하는 포스트에서 따로 언급하도록 하겠습니다.","link":"/2021/01/03/PANDAS-BASIC/"},{"title":"Series &amp; DataFrame Creation","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 Series와 DataFrame 객체를 이용하여 데이터를 분석 및 조작할 수 있습니다. 본 글에서는 Series와 DataFrame을 생성하는 방법에 대해 알아봅니다. 먼저 Pandas와 Numpy 모듈을 import 합니다. 12import numpy as npimport pandas as pd SeriesSeries는 모든 데이터 유형(int, float, str, object 등)을 저장할 수 있는 1차원 데이터 구조입니다. 1차원 Numpy Array와 동일하게 위치(정수)를 기반으로 Indexing과 Slicing을 사용할 수 있다는 점이 동일하지만 인덱스 문자로 지정한 경우 이름(문자열)을 기반으로 Indexing과 Slicing을 할 수 있습니다. Series는 1차원 자료 구조를 기반으로 Series 객체를 만들어 주는 생성자 함수를 이용하여 생성할 수 있습니다. a Python dict, list, tuple an ndarray a scalar value (like 5) 인덱스를 설정하지 않는 경우 기본적으로 위치(정수)가 옵니다. 기존에 사용하는 1차원 자료 구조와는 다르게 데이터가 열을 기준으로 나열됩니다. 123series = pd.Series([1, 2, 3, 4, 5])series 인덱스를 문자로 지정하고 싶은 경우 index 매개변수에 데이터와 동일한 개수를 가지는 1차원 자료 구조를 대입합니다. 123series = pd.Series([1, 2, 3, 4, 5], index=['A', 'B', 'C', 'D', 'E'])series DataFrameDataFrame은 Pandas에서 가장 많이 사용되는 구조로 2차원 데이터 구조로 행과 열이 존재하는 테이블 구조입니다. 많이 사용하는 엑셀의 한 Sheet 또는 SQL의 테이블이라고도 볼 수 있습니다. 또한 DataFrame은 여러개의 Series가 Dict로 묶여 있다고도 볼 수 있습니다. DataFrame은 2차원 자료 구조를 기반으로 DataFrame 객체를 만들어 주는 생성자 함수를 이용하여 생성할 수 있습니다. Dict of 1D ndarrays, lists, dicts, or Series 2-D numpy.ndarray Structured or record ndarray A Series Another DataFrame 행 인덱스와 열 인덱스(컬럼명)를 설정하지 않는 경우 기본적으로 위치(정수)가 옵니다. 1234dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]])dataframe 행 인덱스를 문자로 지정하고 싶은 경우 index 매개 변수를 열 인덱스를 문자로 지정하고 싶은 경우 columns 매개 변수에 동일한 개수를 가지는 1차원 자료 구조를 대입합니다. 1234dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=['A','B','C'])dataframe 1234dataframe = pd.DataFrame( [[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=['A','B','C'], index=['a', 'b', 'c'])dataframe 또한 DataFrame은 Dict를 기반으로도 생성할 수 있습니다. 이때 Dict의 키가 가지고 있는 Value의 개수는 동일해야 하며 Dict의 키가 열 인덱스(컬럼명)로 지정됩니다. 123456dataframe = pd.DataFrame( { 'A' : [1, 2, 3], 'B' : [4, 5, 6], 'C' : [7, 8, 9]})dataframe","link":"/2021/01/02/PANDAS-CREATE/"},{"title":"File I&#x2F;O - CSV","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 다양한 파일을 읽어오거나 저장할 수있습니다. 본 글에서는 CSV파일을 읽어오고 저장하는 방법에 대해 알아봅니다. Pandas에서 지원하는 파일은 다음과 같습니다. Format Type Data Description Reader Writer text CSV read_csv to_csv text Fixed-Width Text File read_fwf text JSON read_json to_json text HTML read_html to_html text Local clipboard read_clipboard to_clipboard binary MS Excel read_excel to_excel binary OpenDocument read_excel binary HDF5 Format read_hdf to_hdf binary Feather Format read_feather to_feather binary Parquet Format read_parquet to_parquet binary ORC Format read_orc binary Msgpack read_msgpack to_msgpack binary Stata read_stata to_stata binary SAS read_sas binary SPSS read_spss binary Python Pickle Format read_pickle to_pickle SQL SQL read_sql to_sql SQL Google BigQuery read_gbq to_gbq 출처 : https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html 이중 CSV 파일을 읽어오고 저장하는 법에 대해 알아봅니다. CSV 파일 형식CSV는 Comma Separated Values의 약자로 표 형태의 데이터를 저장하는 파일 형식입니다. 확장자는 주로 .csv를 사용하며 .txt, .dat와 같은 다른 유형을 사용하기도 합니다. CSV 파일 형식은 한 줄이 한 개의 행에 해당하며, 열 사이에는 쉼표(,)를 넣어 구분하며 쉼표가 아닌 다른 구분자를 사용하는 경우도 존재합니다. 만약 기존의 데이터에 쉼표(,)가 포함 된 경우 다른 형태의 구분자를 사용합니다. ID NAME AGE 0 KIM 10 1 LEE 20 예를들어 위와 같은 테이블을 CSV 파일로 저장하면 다음과 같습니다. CSV 파일 형식123ID,NAME,AGE0,KIM,101,LEE,20 CSV 파일 읽어오기 (기본)Pandas에서 CSV 파일을 읽기 위해서는 read_csv 함수를 이용합니다. 기본적인 사용 방법은 CSV 파일의 경로(이름)을 지정하여 DataFrame 객체로 읽어 올 수 있습니다. csv1.csv123ID,NAME,AGE0,KIM,101,LEE,20 기본 구분자123import pandas as pddf1 = pd.read_csv('csv1.csv')df1 CSV 파일 읽어오기 (구분자 변경)만약 구분자가 쉼표(,)가 아닌 다른 형식이라면 해당 구분자를 sep 파라미터의 인자로 지정합니다. csv2.csv123ID;NAME;AGE0;KIM;101;LEE;20 구분자 ;123import pandas as pddf2 = pd.read_csv('csv2.csv', sep=';')df2 CSV 파일 읽어오기 (인코딩 변경)Pandas에서는 기본적으로 UTF-8을 사용합니다. 만약 CSV 파일이 UTF-8 인코딩이 아닌 경우 파일의 인코딩 유형을 encoding 파라미터의 인자로 지정합니다. 한글이 포함된 데이터의 경우 주로 CP949, EUC-KR과 같은 인코딩을 사용할 수 있습니다.만약 UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xbe in position 0: invalid start byte과 같은 에러가 발생하면 encoding을 변경하면 됩니다. csv3.csv123아이디;이름;나이0;KIM;101;LEE;20 구분자 ;, 인코딩 cp949123import pandas as pddf3 = pd.read_csv('csv3.csv', sep=';', encoding='cp949')df3 CSV 파일 읽어오기 (열 이름 미존재)파일에서 첫번째 행을 열 이름, 컬럼명으로 지정됩니다. 만약 첫번째 행이 열 이름이 아닌 데이터인 경우 None을 header 파라미터의 인자로 전달합니다. csv4.csv120;KIM;101;LEE;20 구분자 ;, 열 이름 미존재123import pandas as pddf4 = pd.read_csv('csv4.csv', sep=';', header=None)df4 header 파라미터의 인자가 None인 경우 names 파라미터의 인자로 전달하여 열 이름을 지정 할 수 있습니다. 구분자 ;, 열 이름 미존재123import pandas as pddf4 = pd.read_csv('csv4.csv', sep=';', header=None, names=['X1', 'X2', 'X3'])df4 만약 열 이름이 여러개의 행으로 설정된 경우 해당 행 번호의 list를 header 파라미터의 인자로 전달합니다. csv5.csv1234ID;NAME;AGEKO;KO;KO0;KIM;101;LEE;20 구분자 ;, 다중 열 이름 존재123import pandas as pddf5 = pd.read_csv('csv5.csv', sep=';', header=[0, 1])df5 또는 마지막 열 이름 행을 지정하여 이전 값을 무시할 수 있습니다. 구분자 ;, 다중 열 이름 존재123import pandas as pddf5 = pd.read_csv('csv5.csv', sep=';', header=1)df5 CSV 파일 읽어오기 (특정 열 선택)CSV의 전체가 아닌 특정 열만 필요한 경우 필요한 열 정보만 usecols 파라미터의 인자로 전달합니다. csv6.csv123456X1;X2;X3;X4;X51,1,1,1,12,2,2,2,23,3,3,3,34,4,4,4,45,5,5,5,5 기본 구분자, 특정 열 선택 (위치)12df6 = pd.read_csv('csv6.csv', usecols=[0, 2])df6 기본 구분자, 특정 열 선택 (이름)12df6 = pd.read_csv('csv6.csv', usecols=['X2', 'X4'])df6 CSV 파일 저장하기DataFrame 객체를 CSV 파일로 저장하기 위해서는 to_csv함수를 이용합니다. 필수적으로 저장할 파일 경로 및 이름을 지정해야합니다. DataFrame 생성데이터 프레임 객체 생성123456789import numpy as npimport pandas as pdindex = pd.date_range('20200101', '20201231', freq='M')columns = [f'X{n:02d}' for n in range(1, 11)]df = pd.DataFrame(np.random.randn(12, 10), index=index, columns=columns)df.loc['합계', :] = df.sum(axis=0)df.loc[:, '합계'] = df.sum(axis=1)df CSV 파일 저장현재 폴더에 data.csv 파일을 생성합니다. 구분자는 ,로 인코딩은 UTF-8로 설정됩니다. 기본 저장1df.to_csv('data.csv') 저장된 결과 확인1pd.read_csv('data.csv') DataFrame 객체의 Index도 포함하여 저장됩니다. 해당 파일을 읽어오면 기존 Index는 Unnamed: 0 으로 지정됩니다. 파일 저장시 index 파라미터의 인자로 False로 지정하면 Index를 포함하지 않습니다. 인덱스 미포함1df.to_csv('data.csv', index=False) 저장된 결과 확인1pd.read_csv('data.csv') 윈도우 Excel 프로그램에서 UTF-8 형식의 CSV 파일을 여는 경우 한글이 깨져보일 수 있습니다. 이는 윈도우 Excel 프로그램 CP949 인코딩을 사용하기 때문입니다. 원하는 인코딩 형식을 encoding 파라미터의 인자로 지정할 수 있습니다. 인덱스 미포함, 인코딩 변경1df.to_csv('data.csv', index=False, encoding='CP949') 저장된 결과 확인1pd.read_csv('data.csv', encoding='CP949')","link":"/2021/01/13/PANDAS-FILEIO-CSV/"},{"title":"File I&#x2F;O - Excel","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 다양한 파일을 읽어오거나 저장할 수있습니다. 본 글에서는 Excel 파일을 읽어오고 저장하는 방법에 대해 알아봅니다. Pandas에서 지원하는 파일은 다음과 같습니다. Format Type Data Description Reader Writer text CSV read_csv to_csv text Fixed-Width Text File read_fwf text JSON read_json to_json text HTML read_html to_html text Local clipboard read_clipboard to_clipboard binary MS Excel read_excel to_excel binary OpenDocument read_excel binary HDF5 Format read_hdf to_hdf binary Feather Format read_feather to_feather binary Parquet Format read_parquet to_parquet binary ORC Format read_orc binary Msgpack read_msgpack to_msgpack binary Stata read_stata to_stata binary SAS read_sas binary SPSS read_spss binary Python Pickle Format read_pickle to_pickle SQL SQL read_sql to_sql SQL Google BigQuery read_gbq to_gbq 출처 : https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html 이중 Excel 파일을 읽어오고 저장하는 법에 대해 알아봅니다. Excel 파일 형식Excel 파일은 통합 문서의 워크 시트에 대한 정보가 포함된 이진 파일로 데이터를 저장할뿐만 여러 서식이 포함됩니다. 테이블 형식의 파일을 저장하며 한 파일의 여러개의 워크 시트를 가질 수 있습니다. Pandas에서는 xlrd, openpyxl와 같은 모듈을 이용하여 Excel 파일을 읽어오거나 저장합니다. Excel 파일 읽어오기 (기본)Pandas에서 CSV 파일을 읽기 위해서는 read_excel 함수를 이용합니다. 기본적인 사용 방법은 Excel 파일의 경로(이름)을 지정하여 DataFrame 객체로 읽어 올 수 있습니다. 123import pandas as pddf = pd.read_excel('excel_1.xlsx')df Excel 파일 읽어오기 (시트 변경)한 엑셀 파일에 여러개의 워크 시트가 있는 경우 기본적으로 가장 처음에 있는 워크 시트를 읽어옵니다. 12df = pd.read_excel('excel_2.xlsx')df 읽어오는 워크시트를 바꾸기 위해서는 sheet_name 파라미터에 인덱스 또는 시트명을 인자로 전달합니다. 시트 변경 (인덱스 이용)12df = pd.read_excel('excel_2.xlsx', sheet_name=1)df 시트 변경 (시트명 이용)12df = pd.read_excel('excel_2.xlsx', sheet_name='2018년 월별 매출')df 인덱스 또는 시트명을 리스트 형태로 전달하면 Dict 형태의 결과가 반환 됩니다. 사용된 인덱스 또는 시트명이 Key, 해당 워크 시트를 DataFrame 객체를 가지는 Value로 지정됩니다. 복수 선택12df = pd.read_excel('excel_2.xlsx', sheet_name=[0, 1])df 모든 워크시트를 읽어오고 싶으면 sheet_name을 None으로 지정합니다. 복수 선택12df = pd.read_excel('excel_2.xlsx', sheet_name=None)df Excel 파일 읽어오기 (열 이름 미존재)Excel 파일도 첫 번째 행을 열 이름(럼명)으로 지정됩니다. 만약 첫 번째 행이 열 이름이 아닌 데이터인 경우 None을 header 파라미터의 인자로 전달합니다. 열 이름 미존재12df = pd.read_excel('excel_3.xlsx')df header 파라미터의 인자가 None인 경우 names 파라미터의 인자로 전달하여 열 이름을 지정 할 수 있습니다. 구분자 ;, 열 이름 미존재12df = pd.read_excel('excel_3.xlsx', header=None, names=['월', '판매량'])df 만약 열 이름이 여러개의 행으로 설정된 경우 해당 행 번호의 list를 header 파라미터의 인자로 전달합니다. 다중 열 이름 존재 (Multi Index)12df = pd.read_excel('excel_3.xlsx', sheet_name=1, header=[0, 1])df 또는 마지막 열 이름의 행을 지정하여 이전 값을 무시할 수 있습니다. 다중 열 이름 존재 (Index)12df = pd.read_excel('excel_3.xlsx', sheet_name=1, header=[1])df Excel 파일 읽어오기 (특정 열 선택)Excel의 전체가 아닌 특정 열만 필요한 경우 필요한 열 정보만 usecols 파라미터의 인자로 전달합니다. 특정 열 선택 (위치)12df = pd.read_excel('excel_4.xlsx', usecols=[1, 3])df 이름으로 열을 선택하기 위해서는 Excel에서 사용하는 문자를 입력합니다. 콜론(:)을 이용하여 특정 범위를 선택하거나 콤마(,)를 이용하여 특정 열만 선택할 수 있습니다. 범위 선택 (이름)12df = pd.read_excel('excel_4.xlsx', usecols='A:B')df 대상 선택 (이름)12df = pd.read_excel('excel_4.xlsx', usecols='A:B')df Excel 파일 저장하기 (단일 워크시트)DataFrame 객체를 Excel 파일로 저장하기 위해서는 to_excel 함수를 이용합니다. 필수적으로 저장할 파일 경로 및 이름을 지정해야 합니다. DataFrame 생성데이터 프레임 객체 생성123456789import numpy as npimport pandas as pdindex = pd.date_range('20200101', '20201231', freq='M')columns = [f'X{n:02d}' for n in range(1, 11)]df = pd.DataFrame(np.random.randn(12, 10), index=index, columns=columns)df.loc['합계', :] = df.sum(axis=0)df.loc[:, '합계'] = df.sum(axis=1)df Excel 파일 저장현재 폴더에 data.xlsx 파일을 생성합니다. 시트명은 ‘2020년 월별 매출’로 저장합니다. 기본 저장1df.to_excel('data.xlsx', sheet_name='2020년 월별 매출') 저장된 결과 확인1pd.read_excel('data.xlsx') DataFrame 객체의 Index도 포함하여 저장됩니다. 해당 파일을 읽어오면 기존 Index는 Unnamed: 0으로 지정됩니다. 파일 저장 시 index 파라미터의 인자로 False로 지정하면 Index를 포함하지 않습니다. 인덱스 미포함1df.to_excel('data.xlsx', sheet_name='2020년 월별 매출', index=False) 저장된 결과 확인1pd.read_excel('data.xlsx') Excel 파일 저장 (복수 워크시트)Excel 파일은 여러 개의 워크시트를 가질 수 있습니다. 만약 한 파일에 여러 개의 워크시트를 지정하기 위해서는 ExcelWriter를 이용합니다. DataFrame 생성데이터 프레임 객체 생성1234567891011121314151617181920import numpy as npimport pandas as pdindex = pd.date_range('20200101', '20201231', freq='M')columns = [f'X{n:02d}' for n in range(1, 11)]df_2020 = pd.DataFrame(np.random.randn(12, 10), index=index, columns=columns)df_2020.loc['합계', :] = df_2020.sum(axis=0)df_2020.loc[:, '합계'] = df_2020.sum(axis=1)index = pd.date_range('20190101', '20191231', freq='M')columns = [f'X{n:02d}' for n in range(1, 11)]df_2019 = pd.DataFrame(np.random.randn(12, 10), index=index, columns=columns)df_2019.loc['합계', :] = df_2019.sum(axis=0)df_2019.loc[:, '합계'] = df_2019.sum(axis=1)index = pd.date_range('20180101', '20181231', freq='M')columns = [f'X{n:02d}' for n in range(1, 11)]df_2018 = pd.DataFrame(np.random.randn(12, 10), index=index, columns=columns)df_2018.loc['합계', :] = df_2018.sum(axis=0)df_2018.loc[:, '합계'] = df_2018.sum(axis=1) ExcelWriter 사용with 구문을 사용하여 자동으로 ExcelWriter 객체가 종료되기 때문에 정상적으로 저장이 됩니다. ExcelWriter 객체를 생성 시 mode을 w(writer)로 지정하는 경우 기존의 파일을 무시하고 새로 생성하며 a(append)로 지정하는 경우 기존의 파일에 추가합니다. 쓰기 모드 (w)1234with pd.ExcelWriter('data2.xlsx', mode='w') as writer: df_2020.to_excel(writer, sheet_name='2020년 매출') df_2019.to_excel(writer, sheet_name='2019년 매출') df_2018.to_excel(writer, sheet_name='2018년 매출') 저장된 결과 확인1pd.read_excel('data2.xlsx', sheet_name=None) 새로운 데이터 프레임 생성12345index = [f'{n:02d}월' for n in range(1, 13)] + ['합계']columns = [f'X{n:02d}' for n in range(1, 11)] + ['합계']data = df_2020.values + df_2019.values + df_2018.valuesdf = pd.DataFrame(data, index=index, columns=columns)df with 구문을 사용하지 않는 경우 ExcelWriter 객체가 종료해야 저장되기 때문에 close 함수를 이용하여 결과를 저장합니다. 수정 모드 (a)123writer = pd.ExcelWriter('data2.xlsx', mode='a')df.to_excel(writer, sheet_name='합계 매출')writer.close() 저장된 결과 확인1pd.read_excel('data2.xlsx', sheet_name=None)","link":"/2021/01/15/PANDAS-FILEIO-EXCEL/"},{"title":"File I&#x2F;O - LAB","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 다양한 파일을 읽어오거나 저장할 수있습니다. 본 글에서는 다양한 형태의 공공데이터 파일을 읽어오고 저장하는 방법에 대해 알아봅니다. 소상공인시장진흥공단_상가(상권)정보공공데이터 포털에서 소상공인 시장 진흥공단은 상가(상권) 정보를 파일 데이터와 오픈 API로 제공하고 있습니다. 소상공인시장진흥공단_상가(상권)정보을 클릭하면 파일 데이터를 다운로드할 수 있는 페이지로 이동합니다. 다운로드 버튼을 클릭하여 압축 파일을 다운로드하고 해제하면 다음과 같이 시도별 파일과 파일 열람 방법 한글 파일이 존재하는 것을 확인할 수 있습니다. 파일 열람 방법 한글 파일을 확인하면 다음과 같습니다. 해당 한글 파일에서 읽고자 하는 상가(상권) 정보 파일의 encoding은 UTF-8이며 구분자는 |(파이프)인 것을 확인할 수 있습니다. 상가(상권)정보 파일 읽기123import pandas as pddf = pd.read_csv('소상공인시장진흥공단_상가(상권)정보_서울_202009.csv', sep='|')df.head() 제공되는 파일을 모두 읽어 통합 파일로 만들고자 하는 경우 다음과 같이 사용할 수 있습니다. 상가(상권)정보 파일 읽기12345678import globcsv_files = glob.glob('*.csv')df = pd.DataFrame()for csv in csv_files: temp = pd.read_csv(csv, sep='|') df = pd.concaat([df, temp], ignore_index=True)df.head() 파일로 저장시 원본과 동일하게 구분자를 |로 지정하는 것을 권장합니다. 상가(상권)정보 파일 읽기1df.to_csv('소상공인시장진흥공단_상가(상권)정보_전국_202009.csv', sep='|') 실거래가 공개시스템국토교통부에서는 아파트, 연립/다세대, 단독/다가구, 오피스텔 등 실거래가와 전월세가 정보를 공개하고 있습니다. 실거래가 공개시스템을 클릭하면 파일 데이터를 다운로드할 수 있는 페이지로 이동합니다. 다운로드한 실거래가 파일을 열어서 확인하면 데이터가 17번째 행에서 시작하는 것을 확인할 수 있습니다. 실거래가 파일 읽기123import pandas as pdapt = pd.read_excel('아파트(매매)_실거래가_20210116194313.xlsx', skiprows=16)apt 만약 대량의 데이터가 필요한 경우 공공데이터 포털에서 오픈 API를 이용하여 데이터를 수집할 수 있습니다.","link":"/2021/01/16/PANDAS-FILEIO-LAB/"},{"title":"데이터 전처리 - Feature Scalining","text":"머신러닝 모델을 올바르게 학습시키고 학습 성능을 개선하기 위해서는 특성의 값을 일정한 수준으로 맞춰주는 Feature scalining 작업이 필요합니다. Feature scalining 작업에선 대표적으로 표준화와 정규화를 사용합니다. 본 글에서는 표준화와 정규화를 적용하는 방법에 대해 알아봅니다. Feature scalining은 매우 중요합니다. 이는 각 특성의 값의 범위가 불일치하고 에러를 낮추기 위해 큰 값을 가지는 특성에 가중치가 편향될 수 있기 때문입니다. 이를 해결하기 위해서는 특성의 값을 일정한 수준으로 맞춰주는 Feature scalining 작업이 필요합니다. 표준화서로 다른 정규분포 사이에 비교 또는 특정 정규분포를 토대로 하여 통계적 추정과 같은 분석 작업을 하기 위해 정규 분포의 분산과 표준편차를 표준에 맞게 통일시키는 방법으로 표준화를 적용하면 각 특성은 평균이 0, 표준 편차가 1로 변환됩니다. $$X_i = { X_i - Mean(X) \\over Std(X)}$$ 또한 표준화를 적용하면 3 시그마 규칙(three-sigma rule)을 이용하여 이상치를 제거할 수 있습니다. 평균에서 양쪽으로 3 표준편차의 범위에 거의 모든 값들(99.7%)이 들어감.즉, -3σ 이하 또는 +3σ 이상인 데이터를 이상치로 가정할 수 있음 scikit-learn에서 표준화를 적용하기 위해서는 preprocessing의 StandardScaler 함수를 이용합니다. 모듈 설정모듈 설정123import pandas as pdimport matplotlib.pyplot as pltfrom sklearn.datasets import load_iris 데이터 불러오기iris 데이터 불러오기12iris = load_iris()iris 표준화 적용 전기술 통계 요약12before = pd.DataFrame(iris['data'], columns=iris['feature_names'])before.describe() Boxplot12plt.boxplot(before)plt.show() 표준화 적용표준화 적용12scaler = StandardScaler()after = scaler.fit_transform(before) 표준화 적용 후기술 통계 요약12after = pd.DataFrame(after, columns=iris['feature_names'])after.describe() Boxplot12plt.boxplot(after)plt.show() 표준화 적용으로 인해 describe 함수로 계산된 특성별 mean과 std 값이 0과 1에 가까운 것을 확인할 수 있으며 BoxPlot에서 표현되는 주황색 실선(Median, 50%)이 비슷한 위치에 있는 것을 확인할 수 있습니다. 또한 아래의 코드 실행을 통해 분포의 중심(평균)이 0인 것을 확인할 수 있습니다. Boxplot12345678910111213plt.figure(figsize=(16, 12))cnt = 1for feature_name in iris['feature_names']: plt.subplot(2, 4, cnt) sns.distplot(before[feature_name]) plt.title(f'Before_{feature_name}') cnt += 1 plt.subplot(2, 4, cnt) sns.distplot(after[feature_name]) plt.title(f'After_{feature_name}') cnt += 1plt.show() 정규화서로 다른 특징 값을 같은 단위로 통일시키기 위해 사용하는 방법으로 다양한 정규화 방법이 있지만 최소-최대 정규화를 가장 많이 사용합니다. 각 특성의 값을 0에서 1사이로 데이터가 변환되며 최댓값을 1, 최솟값을 0으로 변환됩니다. $$X_i = { X_i - MIN(X) \\over MAX(X) - MIN(X)}$$ 또한 정규화를 적용하면 데이터 타입이 float로 변환되어 머신러닝 또는 딥러닝의 학습 속도가 빨라집니다. 단, 최솟값과 최댓값이 고정되어있는 경우 사용할 수 있으며 주로 이미지에서 많이 사용합니다. scikit-learn에서 표준화를 적용하기 위해서는 preprocessing의 MinMaxScaler 함수를 이용합니다. 정규화 적용 전기술 통계 요약12before = pd.DataFrame(iris['data'], columns=iris['feature_names'])before.describe() Boxplot12plt.boxplot(before)plt.show() 정규화 적용정규화 적용12scaler = MinMaxScaler()after = scaler.fit_transform(before) 정규화 적용 후기술 통계 요약12after = pd.DataFrame(after, columns=iris['feature_names'])after.describe() Boxplot12plt.boxplot(after)plt.show() 정규화 적용으로 인해 describe 함수로 계산된 특성별 min과 max 값이 0과 1로 변환된 것을 확인할 수 있으며 BoxPlot에서 표현되는 Y축의 값 범위가 0에서 1 사이인 것을 확인할 수 있습니다. 주의할 점예제와 같이 전체 데이터를 변환한다는 것은 미래에 발생한 데이터를 미리 알고 있다는 의미로 실제 머신러닝 프로젝트에서는 적용하기 어려운 문제가 있습니다. 미래에 발생할 데이터의 평균, 표준편차, 최솟값, 최댓값을 알 수 없기 때문입니다. 이를 미리 알아보기 위해서 데이터를 분할 후 학습 데이터를 기준으로 평가 데이터를 변환하는 방법을 사용해볼 수 있습니다. 데이터 분할12from sklearn.model_selection import train_test_splitx_train, x_test, y_train, y_test = train_test_split(iris['data'], iris['target']) BoxPlot12345678plt.figure(figsize=(10, 5))plt.subplot(1, 2, 1)plt.boxplot(x_train)plt.title('정규화 전 - 학습 데이터')plt.subplot(1, 2, 2)plt.boxplot(x_test)plt.title('정규화 전 - 평가 데이터')plt.show() 데이터 변환123scaler = MinMaxScaler()x_train_scaled = scaler.fit_transform(x_train)x_test_scaled = scaler.transform(x_test) BoxPlot12345678plt.figure(figsize=(10, 5))plt.subplot(1, 2, 1)plt.boxplot(x_train_scaled)plt.title('정규화 후 - 학습 데이터')plt.subplot(1, 2, 2)plt.boxplot(x_test_scaled)plt.title('정규화 후 - 평가 데이터')plt.show() 특히 최소 최대 정규화의 경우 새로운 데이터가 기존 변환된 데이터의 최솟값과 최댓값보다 더 큰 수를 가지는 경우 문제가 될 수 있습니다. 데이터 변환123data = [[1], [2], [3], [4], [5]]scaler = MinMaxScaler()scaler.fit_transform(data) 새로운 데이터 변환1scaler.transform([[0], [3], [6]]) 따라서 최소 최대 정규화는 데이터의 값 범위가 고정된 경우 적절하지만 값 범위가 변화하는 경우 다음과 같은 방법을 고려해볼수있습니다. 새롭게 발생한 데이터를 포함하여 데이터를 정규화 후 재학습 데이터 변환의 기준만 변경 표준화 적용","link":"/2021/01/17/ML-SKLEARN-02/"},{"title":"Series &amp; DataFrame Indexer","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 Series와 DataFrame 객체를 이용하여 데이터를 분석 및 조작할 수 있습니다. 본 글에서는 Series와 DataFrame에서 데이터를 선택하는 방법에 대해 알아봅니다. 인덱서의 사용 이유Pandas의 Series와 DataFrame 모두 대괄호([인덱스])를 이용하여 데이터를 가지고 올 수 있습니다. 하지만 Series의 경우 Indexing과 Slicing 모두 행을 대상으로 데이터를 가지고 오지만 DataFrame의 경우 Indexing은 열, Slicing은 행을 대상으로 동작합니다. Numpy Array와 통일성 있게 데이터를 가지고 오기 위해서는 인덱서(Indexer)를 이용합니다. Pandas에는 loc, iloc, at, iat 인덱서가 존재합니다. loc 인덱서loc 인덱서는 labels 즉 이름(문자열)을 기반으로 데이터에 접근할 수 있는 인덱서입니다. loc 인덱서는 다음과 같이 사용합니다. 데이터프레임변수이름.loc[행 인덱스, 열 인덱스] loc 인덱서를 사용하는 경우 행 또는 열 인덱스에는 다음과 같은 형식만 올 수 있습니다. 행 또는 열 이름(문자열) 행 또는 열 이름(문자열)의 리스트 행 또는 열 이름(문자열)의 슬라이싱 마지막 인덱스를 포함 Boolean List, 1D Array, Series DataFrame 불가 행을 대상으로만 데이터를 선택하고자 하는 경우 열 인덱스는 생략할 수 있습니다. 데이터 프레임 생성12345import numpy as npimport pandas as pddf = pd.DataFrame(np.arange(9).reshape(3, 3), index=['A', 'B', 'C'], columns=['X1', 'X2', 'X3'])df 단일 선택문자열을 이용하여 단일 행에 접근할 수 있습니다. 단일 행 접근1df.loc['A'] 단일 열 접근1df.loc[:, 'A'] 범위 선택슬라이싱을 이용하여 순차적인 다중 행에 접근할 수 있습니다. 마지막 인덱스를 포함합니다. 범위 행 접근1df.loc['A':'C'] 범위 열 접근1df.loc[:, 'X1':'X3'] 다중 선택리스트 형태로 행 인덱스를 지정하여 비순차적인 다중 행에 접근할 수 있습니다. 다중 행 접근1df.loc[['A', 'C']] 다중 열 접근1df.loc[:, ['X1', 'X3']] 조건 선택Boolean Series를 이용하여 True 값을 가지는 Index만 선택할 수 있습니다. 행 인덱스를 가지는 Boolean Sereis 이용1df.loc[ df['A'] % 2 == 0 ] 행 인덱스를 가지는 Boolean Sereis 이용1df.loc[:, df.columns != 'X2'] 값 수정인덱서를 이용하여 특정 데이터에 접근한다는 의미는 데이터를 조회할 수 있지만 데이터를 수정할 수도 있습니다. 인덱서를 이용하여 데이터를 선택 후 값을 대입하면 데이터가 수정이 됩니다. 대입하는 값이 1개인 경우 선택된 위치에 모두 같은 값이 대입됩니다. 단일 값 대입12df.loc['C'] = -1df 대입하는 값이 선택된 데이터와 개수가 같은 경우 위치대로 대입됩니다. 단, 대입되는 값이 Sereis인 경우 인덱스 별로 대입됩니다. 다중 값 대입12df.loc['C'] = [1, 2, 3]df 다중 값 대입12df.loc[:, 'X3'] = pd.Series([10, 10, 10], index=['B', 'C', 'D'])df iloc 인덱서iloc 인덱서는 integer 즉 위치(숫자)를 기반으로 데이터에 접근할 수 있는 인덱서입니다. iloc 인덱서는 다음과 같이 사용합니다. 데이터프레임변수이름.iloc[행 인덱스, 열 인덱스] loc 인덱서를 사용하는 경우 행 또는 열 인덱스에는 다음과 같은 형식만 올 수 있습니다. 행 또는 열 위치(정수) 행 또는 열 위치(정수)의 리스트 행 또는 열 위치(정수)의 슬라이싱 마지막 인덱스를 미포함 Boolean List, 1D Array Sereis 불가 행을 대상으로만 데이터를 선택하고자 하는 경우 열 인덱스는 생략할 수 있습니다. 데이터 프레임 생성12345import numpy as npimport pandas as pddf = pd.DataFrame(np.arange(9).reshape(3, 3), index=['A', 'B', 'C'], columns=['X1', 'X2', 'X3'])df 단일 선택문자열을 이용하여 단일 행에 접근할 수 있습니다. 단일 행 접근1df.iloc[0] 단일 열 접근1df.iloc[:, -1] 범위 선택슬라이싱을 이용하여 순차적인 다중 행에 접근할 수 있습니다. 마지막 인덱스를 미포함합니다. 범위 행 접근1df.iloc[1:] 범위 열 접근1df.iloc[:, 1:] 다중 선택리스트 형태로 행 인덱스를 지정하여 비순차적인 다중 행에 접근할 수 있습니다. 다중 행 접근1df.iloc[[0, 2]] 다중 열 접근1df.iloc[[0, 2], [1, 2]] 조건 선택Boolean Series를 이용하여 True 값을 가지는 Index만 선택할 수 있습니다. 행 인덱스를 가지는 Boolean List 이용1df.iloc[[True, False, True]] 행 인덱스를 가지는 Boolean List 이용1df.iloc[:, [True, False, True]] 값 수정인덱서를 이용하여 특정 데이터에 접근한다는 의미는 데이터를 조회할 수 있지만 데이터를 수정할 수도 있습니다. 인덱서를 이용하여 데이터를 선택 후 값을 대입하면 데이터가 수정이 됩니다. 대입하는 값이 1개인 경우 선택된 위치에 모두 같은 값이 대입됩니다. 단일 값 대입12df.iloc[-1] = -1df 대입하는 값이 선택된 데이터와 형태가 같은 경우 위치대로 대입됩니다. 다중 값 대입1df.iloc[:2, :2] = [[1, 2], [3, 4]] 단, 대입되는 값이 Sereis인 경우 인덱스 별로 대입됩니다. 다중 값 대입12df.iloc[-1] = pd.Series([10, 10, 10], index=['X2', 'X3', 'X4'])df at, iat 인덱서at 인덱서는 loc 인덱서와 동일하게 이름(문자열)을 기반으로 동작하며 iat 인덱서 iloc 인덱서와 동일하기 위치(정수)를 기반으로 데이터에 접근할 수 인덱서입니다. loc, iloc 인덱서는 복수개의 데이터에 접근할 수 있지만 at, iat 인덱서는 1개의 데이터에 접근할 수 있다는 차이점이 존재합니다. 규모가 큰 데이터 프레임에서 at 또는 iat 인덱서를 사용하는 경우 loc 또는 iloc 인덱서를 사용하는 경우보다 빠르게 데이터를 접근할 수 있습니다. 데이터 생성12345import numpy as npimport pandas as pddf = pd.DataFrame(np.arange(9).reshape(3, 3), index=['A', 'B', 'C'], columns=['X1', 'X2', 'X3'])df 성능 비교loc 단일 값 접근 속도1%timeit df.loc['index100', 'columns100'] at 단일 값 접근 속도1%timeit df.at['index100', 'columns100'] iloc 단일 값 접근 속도1%timeit df.iloc[100, 100] iat 단일 값 접근 속도1%timeit df.iat[100, 100] 실습loc, iloc 인덱서를 이용하여 데이터를 선택하고 수정하는 작업은 데이터 분석 과정에서 필수적이다고 할 수 있습니다. 그중에서도 loc 인덱서의 사용법을 제대로 숙지하고 있다면 데이터 처리에 많은 이점이 있습니다.","link":"/2021/01/06/PANDAS-SELECTION/"},{"title":"Apple Silicon 개발 환경 설정","text":"현재 사용 중인 M1 MacMini와 M1 MacBook Pro에서 다음과 같은 초기 작업을 진행합니다. Oh-My-Zsh 설정 Brew 설치 (Rosetta2 이용) Tensorflow 설치 Pandas &amp; Jupyter 설치 (Rosetta2 이용) Zsh 설정12345sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)&quot;git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestionsgit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting .zshrc1plugins=(git zsh-autosuggestions zsh-syntax-highlighting) Brew 설치1arch -x86_64 /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)&quot; .zshrc1alias ibrew=&quot;arch -x86_64 brew&quot; Tensorflow 설치1/bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/apple/tensorflow_macos/master/scripts/download_and_install.sh)&quot; SpaceVim 설치1curl -sLf https://spacevim.org/install.sh | bash 로제타 사용 설정 pip 업데이트1pip3 install --upgrade pip --user Pandas 설치1pip3 install pandas or pip3 install pandas --user Seaborn 설치1pip3 install seaborn or pip3 install seaborn --user Seaborn 설치1pip3 install sklearn or pip3 install sklearn --user Jupyter Lab 설치1sudo -H pip3 install jupyterlab Tensorflow Kernel 추가1python3 -m ipykernel install --user --name tensorflow_macos_venv --display-name &quot;tf&quot;","link":"/2020/12/29/SETTING-APPLE-M1/"},{"title":"Jupyter Notebook 폰트 변경","text":"Jupyter Notebook은 웹 브라우저 환경에서 Python 코드를 작성하고 실행할 수 있는 개발도구입니다. 본 글에서는 Jupyter Notebook의 폰트를 변경하는 방법에 대해 알아봅니다. 운영체제, 웹 브라우저에 따라 Jupyter Notebook에서의 Font가 다르게 나오고 글씨를 키우기 위해서는 웹 브라우저의 글꼴 크기를 키워야 하는 단점이 존재합니다. custom.css를 적용하여 이를 해결할 수 있습니다. custom.css를 적용하기 위해서는 사용자 홈 폴더에 존재하는 .jupyter 폴더로 이동하여 custom 폴더를 생성합니다. 운영체제별 경로는 다음과 같습니다. Window C:\\\\Users\\\\{사용자명}\\\\.jupyter/custom Mac /Users/{사용자명}/.jupyter/custom Linux /home/{사용자명}/.jupyter/custom 이후 해당 폴더에 변경할 내용이 담긴 custom.css 파일을 이동 후 Jupyter Notebook을 실행합니다. 현재 적용된 CSS의 경우 크롬 웹 브라우저 사용을 권장합니다. 변경하고자 하는 폰트는 font-family, 크기는 font-size를 지정하며 line-height는 줄 간격을 의미합니다. .jupyter/custom/custom.css123456789101112131415161718192021222324252627282930div.CodeMirror,div.CodeMirror pre { font-family: D2Coding; font-size: 18pt; line-height: 140%;} /* 코드 입력창 */.output_result pre { font-family: D2Coding; font-size: 16pt; line-height: 120%;} /* 출력 결과 */.output_stdout pre { font-family: D2Coding; font-size: 16pt; line-height: 120%;} /* 출력 결과 */.text_cell_render { font-family: D2Coding; font-size: 12pt;} /* 마크 다운 */table.dataframe { font-family: D2Coding; font-size: 12pt;} /* 데이터 프레임 출력 결과 */ 본 글에서 사용한 D2 Coding 폰트는 네이버에서 개발하였으며 나눔 바른바른고딕을 바탕으로 합니다. 자세한 설명은 여기에서 확인할 수 있습니다. 개인적으로 D2 Coding을 사용하는 이유는 특수 기호(|), 한글(ㅣ), 대문자 i(I), 소문자 L(l)를 확실하게 구분할 수 있어서입니다. 출처 : https://github.com/naver/d2codingfont","link":"/2021/01/27/SETTING-JUPYTER-01/"},{"title":"Jupyter Notebook 환경 설정","text":"Jupyter Notebook은 웹 브라우저 환경에서 Python 코드를 작성하고 실행할 수 있는 개발도구입니다. 본 글에서는 Jupyter Notebook의 환경 설정에 대해 알아봅니다.","link":"/2021/01/27/SETTING-JUPYTER-02/"},{"title":"Apple Silicon Table Desktop 설치","text":"현재(2021.01.07) M1 Mac에서 최신 버전(2020.4)의 Tableau Desktop 설치 시 에러가 발생하며 2020.3 버전대를 설치하면 정상적으로 이용 할 수 있습니다. 설치 시 주의 사항현재 Tableau Desktop의 가장 최신 버전은 2020.4 입니다. M1 Mac에서 2020.4 버전을 설치하려는 경우 다음과 같이 에러를 확인할 수 있습니다. 하지만 2020.3 버전의 Tableau Desktop 설치를 하면 정상적으로 설치할 수 있습니다. 다운로드2020.3 버전의 Tableau Desktop 설치를 위해서 링크를 클릭하면 과거 버전의 Tableau Desktop 설치 파일을 다운로드할 수 있는 페이지로 이동합니다. 2020.3.4버전을 선택 후 스크롤을 아래로 내려 Mac 버전의 Tableau 설치파일을 다운로드할 수 있습니다. 설치 과정2020.3 버전대의 설치 파일을 실행하면 정상적으로 설치 프로그램이 실행됩니다. 다운로드한 설치 파일을 실행 후 라이선스 동의 후 설치 과정을 계속 진행합니다. 비밀번호 또는 Touch ID 인증 후 설치 과정을 계속 진행합니다. 설치가 완료되면 Tableau Desktop 앱을 실행합니다. 만약 제품키가 없는 경우 체험판(14일) 활성화를 제품키가 있는 경우 인증 후 정상적으로 이용 할 수 있습니다.","link":"/2021/01/07/SETTING-TABLEAU-M1/"},{"title":"TensorFlow Certificate 준비 및 시험 과정","text":"Tensorflow CertificateTensorFlow Certificate은 딥러닝 및 머신러닝(ML) 문제를 해결할 수 있는 능력을 입증하기 위한 일종의 자격증입니다. 시험은 5시간 동안 진행되며 크게 DNN, CNN, RNN 모델 생성 및 튜닝에 대해 할 수 있으면 합격할 수 있는 난이도입니다. 시험 환경은 PyCharm IDE 기반으로 TensorFlow Exam 플러그인을 설치하여 시험이 진행되며 최신 버전인 2020.3에선 지원하지 않으니 꼭 2020.2 버전을 설치 해야 합니다. 2020년 12월 25일 기준 이외 자세한 사항은 TensorFlow 개발자 인증을 통해 확인 할 수 있습니다. 시험 준비 사항앞서 이야기 한 대로 시험은 총 5시간 동안 진행되며 5가지 유형(Category)의 모델을 구현하고 제출 하면 됩니다. 5가지 유형은 다음과 같습니다. (해당 내용은 개발자 인증 프로그램 핸드북에서 확인 할 수 있습니다.) Category 1: Basic / Simple model Category 2: Model from learning dataset Category 3: Convolutional Neural Network with real-world image dataset Category 4: NLP Text Classification with real-world text dataset Category 5: Sequence Model with real-world numeric dataset 시험의 경우 구글에서 친절히 관련 강의를 소개 하고 있습니다. DeepLearning.AI TensorFlow 개발자 전문 자격증 Intro to TensorFlow for Deep Learning 또한 이외 국내 강의는 다음과 같이 있습니다 텐서플로우 자격증 취득 (온라인) 만약, TensorFlow에 대해 처음 접하는 경우 텐서플로우 자격증 취득 (온라인)를 추천합니다. 시험 과정응시 전시험 응시 전 사전에 Google TensorFlow Developer Certificate에서 시험을 결제 합니다. 시험 응시료는 100$이며 신분증(여권, 면허증)과 본인 인증 사진(웹캠) 업로드 과정이 포함되어 있습니다. 신청 과정을 다 끝내면 접수한 내용을 검토하는 과정으로 인해 시간이 소모됩니다. 이로 인해 바로 시험을 진행 할 수 없으니 미리 결제를 한 후 시험 응시 전 Redeem을 활성화 하는 것을 추천합니다. 만약 과정이 다 끝나면 다음과 같이 정보를 확인 할 수 있으며 Redeem 이라는 버튼이 존재합니다. 해당 버튼을 누르면 시험이 응시되니 꼭 시험 볼 준비가 완료 된 이후 클릭 하시는걸 추천드립니다. 저의 경우 이미 응시한 시험 정보로 Redeem 버튼은 보이지 않는 상태입니다. 응시 환경 구축시험 환경은 PyCharm IDE 기반으로 TensorFlow Exam 플러그인을 설치하여 시험이 진행되며 최신 버전인 2020.3에선 지원하지 않으니 꼭 2020.2 버전을 설치 해야 합니다. PyCharm 2020.2 버전은 다운로드를 클릭하면 받을 수 있습니다. 또한 Anaconda Platform을 이용하는것을 추천합니다. 응시 후5가지 유형(Category)의 모델을 구현 후 제출하면 시험이 종료 되며 거의 즉시 결과가 이메일로 옵니다. (약 제출 후 10초 이내) 또한, 인증서는 최대 2주가 소요가 됩니다. (저의 경우 3일만에 메일로 전달 받았습니다)","link":"/2021/01/06/TensorFlow-Certificate/"},{"title":"법정동 코드(시군구)","text":"본 글에서는 시군구 단위의 법정동 코드를 제공합니다. 서울특별시서울특별시12# 서울특별시[['강남구', '11680'], ['강동구', '11740'], ['강북구', '11305'], ['강서구', '11500'], ['관악구', '11620'], ['광진구', '11215'], ['구로구', '11530'], ['금천구', '11545'], ['노원구', '11350'], ['도봉구', '11320'], ['동대문구', '11230'], ['동작구', '11590'], ['마포구', '11440'], ['서대문구', '11410'], ['서초구', '11650'], ['성동구', '11200'], ['성북구', '11290'], ['송파구', '11710'], ['양천구', '11470'], ['영등포구', '11560'], ['용산구', '11170'], ['은평구', '11380'], ['종로구', '11110'], ['중구', '11140'], ['중랑구', '11260']] 서울특별시 법정동코드 강남구 11680 강동구 11740 강북구 11305 강서구 11500 관악구 11620 광진구 11215 구로구 11530 금천구 11545 노원구 11350 도봉구 11320 동대문구 11230 동작구 11590 마포구 11440 서대문구 11410 서초구 11650 성동구 11200 성북구 11290 송파구 11710 양천구 11470 영등포구 11560 용산구 11170 은평구 11380 종로구 11110 중구 11140 중랑구 11260 부산광역시부산광역시12# 부산광역시[['강서구', '26440'], ['금정구', '26410'], ['기장군', '26710'], ['남구', '26290'], ['동구', '26170'], ['동래구', '26260'], ['부산진구', '26230'], ['북구', '26320'], ['사상구', '26530'], ['사하구', '26380'], ['서구', '26140'], ['수영구', '26500'], ['연제구', '26470'], ['영도구', '26200'], ['중구', '26110'], ['해운대구', '26350']] 부산광역시 법정동코드 강서구 26440 금정구 26410 기장군 26710 남구 26290 동구 26170 동래구 26260 부산진구 26230 북구 26320 사상구 26530 사하구 26380 서구 26140 수영구 26500 연제구 26470 영도구 26200 중구 26110 해운대구 26350 대구광역시대구광역시12# 대구광역시[['남구', '27200'], ['달서구', '27290'], ['달성군', '27710'], ['동구', '27140'], ['북구', '27230'], ['서구', '27170'], ['수성구', '27260'], ['중구', '27110']] 대구광역시 법정동코드 남구 27200 달서구 27290 달성군 27710 동구 27140 북구 27230 서구 27170 수성구 27260 중구 27110 인천광역시인천광역시12# 인천광역시[['강화군', '28710'], ['계양구', '28245'], ['남동구', '28200'], ['동구', '28140'], ['미추홀구', '28177'], ['부평구', '28237'], ['서구', '28260'], ['연수구', '28185'], ['옹진군', '28720'], ['중구', '28110']] 인천광역시 법정동코드 강화군 28710 계양구 28245 남동구 28200 동구 28140 미추홀구 28177 부평구 28237 서구 28260 연수구 28185 옹진군 28720 중구 28110 광주광역시광주광역시12# 광주광역시[['광산구', '29200'], ['남구', '29155'], ['동구', '29110'], ['북구', '29170'], ['서구', '29140']] 광주광역시 법정동코드 광산구 29200 남구 29155 동구 29110 북구 29170 서구 29140 대전광역시대전광역시12# 대전광역시[['대덕구', '30230'], ['동구', '30110'], ['서구', '30170'], ['유성구', '30200'], ['중구', '30140']] 대전광역시 법정동코드 대덕구 30230 동구 30110 서구 30170 유성구 30200 중구 30140 울산광역시울산광역시12# 울산광역시[['남구', '31140'], ['동구', '31170'], ['북구', '31200'], ['울주군', '31710'], ['중구', '31110']] 울산광역시 법정동코드 남구 31140 동구 31170 북구 31200 울주군 31710 중구 31110 세종특별자치시세종특별자치시12# 세종특별자치시[['세종특별자치시', '36110']] 세종특별자치시 법정동코드 세종특별자치시 36110 경기도경기도12# 경기도[['가평군', '41820'], ['고양시 덕양구', '41281'], ['고양시 일산동구', '41285'], ['고양시 일산서구', '41287'], ['과천시', '41290'], ['광명시', '41210'], ['광주시', '41610'], ['구리시', '41310'], ['군포시', '41410'], ['김포시', '41570'], ['남양주시', '41360'], ['동두천시', '41250'], ['부천시', '41190'], ['성남시 분당구', '41135'], ['성남시 수정구', '41131'], ['성남시 중원구', '41133'], ['수원시 권선구', '41113'], ['수원시 영통구', '41117'], ['수원시 장안구', '41111'], ['수원시 팔달구', '41115'], ['시흥시', '41390'], ['안산시 단원구', '41273'], ['안산시 상록구', '41271'], ['안성시', '41550'], ['안양시 동안구', '41173'], ['안양시 만안구', '41171'], ['양주시', '41630'], ['양평군', '41830'], ['여주시', '41670'], ['연천군', '41800'], ['오산시', '41370'], ['용인시 기흥구', '41463'], ['용인시 수지구', '41465'], ['용인시 처인구', '41461'], ['의왕시', '41430'], ['의정부시', '41150'], ['이천시', '41500'], ['파주시', '41480'], ['평택시', '41220'], ['포천시', '41650'], ['하남시', '41450'], ['화성시', '41590']] 경기도 법정동코드 가평군 41820 고양시 덕양구 41281 고양시 일산동구 41285 고양시 일산서구 41287 과천시 41290 광명시 41210 광주시 41610 구리시 41310 군포시 41410 김포시 41570 남양주시 41360 동두천시 41250 부천시 41190 성남시 분당구 41135 성남시 수정구 41131 성남시 중원구 41133 수원시 권선구 41113 수원시 영통구 41117 수원시 장안구 41111 수원시 팔달구 41115 시흥시 41390 안산시 단원구 41273 안산시 상록구 41271 안성시 41550 안양시 동안구 41173 안양시 만안구 41171 양주시 41630 양평군 41830 여주시 41670 연천군 41800 오산시 41370 용인시 기흥구 41463 용인시 수지구 41465 용인시 처인구 41461 의왕시 41430 의정부시 41150 이천시 41500 파주시 41480 평택시 41220 포천시 41650 하남시 41450 화성시 41590 강원도강원도12# 강원도[['강릉시', '42150'], ['고성군', '42820'], ['동해시', '42170'], ['삼척시', '42230'], ['속초시', '42210'], ['양구군', '42800'], ['양양군', '42830'], ['영월군', '42750'], ['원주시', '42130'], ['인제군', '42810'], ['정선군', '42770'], ['철원군', '42780'], ['춘천시', '42110'], ['태백시', '42190'], ['평창군', '42760'], ['홍천군', '42720'], ['화천군', '42790'], ['횡성군', '42730']] 강원도 법정동코드 강릉시 42150 고성군 42820 동해시 42170 삼척시 42230 속초시 42210 양구군 42800 양양군 42830 영월군 42750 원주시 42130 인제군 42810 정선군 42770 철원군 42780 춘천시 42110 태백시 42190 평창군 42760 홍천군 42720 화천군 42790 횡성군 42730 충청북도충청북도12# 충청북도[['괴산군', '43760'], ['단양군', '43800'], ['보은군', '43720'], ['영동군', '43740'], ['옥천군', '43730'], ['음성군', '43770'], ['제천시', '43150'], ['증평군', '43745'], ['진천군', '43750'], ['청주시 상당구', '43111'], ['청주시 서원구', '43112'], ['청주시 청원구', '43114'], ['청주시 흥덕구', '43113'], ['충주시', '43130']] 충청북도 법정동코드 괴산군 43760 단양군 43800 보은군 43720 영동군 43740 옥천군 43730 음성군 43770 제천시 43150 증평군 43745 진천군 43750 청주시 상당구 43111 청주시 서원구 43112 청주시 청원구 43114 청주시 흥덕구 43113 충주시 43130 충청남도충청남도12# 충청남도[['계룡시', '44250'], ['공주시', '44150'], ['금산군', '44710'], ['논산시', '44230'], ['당진시', '44270'], ['보령시', '44180'], ['부여군', '44760'], ['서산시', '44210'], ['서천군', '44770'], ['아산시', '44200'], ['예산군', '44810'], ['천안시 동남구', '44131'], ['천안시 서북구', '44133'], ['청양군', '44790'], ['태안군', '44825'], ['홍성군', '44800']] 충청남도 법정동코드 계룡시 44250 공주시 44150 금산군 44710 논산시 44230 당진시 44270 보령시 44180 부여군 44760 서산시 44210 서천군 44770 아산시 44200 예산군 44810 천안시 동남구 44131 천안시 서북구 44133 청양군 44790 태안군 44825 홍성군 44800 전라북도전라북도12# 전라북도[['고창군', '45790'], ['군산시', '45130'], ['김제시', '45210'], ['남원시', '45190'], ['무주군', '45730'], ['부안군', '45800'], ['순창군', '45770'], ['완주군', '45710'], ['익산시', '45140'], ['임실군', '45750'], ['장수군', '45740'], ['전주시 덕진구', '45113'], ['전주시 완산구', '45111'], ['정읍시', '45180'], ['진안군', '45720']] 전라북도 법정동코드 고창군 45790 군산시 45130 김제시 45210 남원시 45190 무주군 45730 부안군 45800 순창군 45770 완주군 45710 익산시 45140 임실군 45750 장수군 45740 전주시 덕진구 45113 전주시 완산구 45111 정읍시 45180 진안군 45720 전라남도전라남도12# 전라남도[['강진군', '46810'], ['고흥군', '46770'], ['곡성군', '46720'], ['광양시', '46230'], ['구례군', '46730'], ['나주시', '46170'], ['담양군', '46710'], ['목포시', '46110'], ['무안군', '46840'], ['보성군', '46780'], ['순천시', '46150'], ['신안군', '46910'], ['여수시', '46130'], ['영광군', '46870'], ['영암군', '46830'], ['완도군', '46890'], ['장성군', '46880'], ['장흥군', '46800'], ['진도군', '46900'], ['함평군', '46860'], ['해남군', '46820'], ['화순군', '46790']] 전라남도 법정동코드 강진군 46810 고흥군 46770 곡성군 46720 광양시 46230 구례군 46730 나주시 46170 담양군 46710 목포시 46110 무안군 46840 보성군 46780 순천시 46150 신안군 46910 여수시 46130 영광군 46870 영암군 46830 완도군 46890 장성군 46880 장흥군 46800 진도군 46900 함평군 46860 해남군 46820 화순군 46790 경상북도경상북도12# 경상북도[['경산시', '47290'], ['경주시', '47130'], ['고령군', '47830'], ['구미시', '47190'], ['군위군', '47720'], ['김천시', '47150'], ['문경시', '47280'], ['봉화군', '47920'], ['상주시', '47250'], ['성주군', '47840'], ['안동시', '47170'], ['영덕군', '47770'], ['영양군', '47760'], ['영주시', '47210'], ['영천시', '47230'], ['예천군', '47900'], ['울릉군', '47940'], ['울진군', '47930'], ['의성군', '47730'], ['청도군', '47820'], ['청송군', '47750'], ['칠곡군', '47850'], ['포항시 남구', '47111'], ['포항시 북구', '47113']] 경상북도 법정동코드 경산시 47290 경주시 47130 고령군 47830 구미시 47190 군위군 47720 김천시 47150 문경시 47280 봉화군 47920 상주시 47250 성주군 47840 안동시 47170 영덕군 47770 영양군 47760 영주시 47210 영천시 47230 예천군 47900 울릉군 47940 울진군 47930 의성군 47730 청도군 47820 청송군 47750 칠곡군 47850 포항시 남구 47111 포항시 북구 47113 경상남도경상남도12# 경상남도[['거제시', '48310'], ['거창군', '48880'], ['고성군', '48820'], ['김해시', '48250'], ['남해군', '48840'], ['밀양시', '48270'], ['사천시', '48240'], ['산청군', '48860'], ['양산시', '48330'], ['의령군', '48720'], ['진주시', '48170'], ['창녕군', '48740'], ['창원시 마산합포구', '48125'], ['창원시 마산회원구', '48127'], ['창원시 성산구', '48123'], ['창원시 의창구', '48121'], ['창원시 진해구', '48129'], ['통영시', '48220'], ['하동군', '48850'], ['함안군', '48730'], ['함양군', '48870'], ['합천군', '48890']] 경상남도 법정동코드 거제시 48310 거창군 48880 고성군 48820 김해시 48250 남해군 48840 밀양시 48270 사천시 48240 산청군 48860 양산시 48330 의령군 48720 진주시 48170 창녕군 48740 창원시 마산합포구 48125 창원시 마산회원구 48127 창원시 성산구 48123 창원시 의창구 48121 창원시 진해구 48129 통영시 48220 하동군 48850 함안군 48730 함양군 48870 합천군 48890 제주특별자치도제주특별자치도12# 제주특별자치도[['서귀포시', '50130'], ['제주시', '50110']] 제주특별자치도 법정동코드 서귀포시 50130 제주시 50110","link":"/2021/01/28/DATA-APT-02-1/"},{"title":"Function","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 Series와 DataFrame 객체를 이용하여 데이터를 분석 및 조작할 수 있습니다. 본 글에서는 Pandas에서 제공하는 함수들의 사용 방법에 대해 알아봅니다. 데이터 생성12345678import numpy as npimport pandas as pdnp.random.seed(2021)df = pd.DataFrame( np.random.randint(1000, 2000, 36).reshape(-1, 3), index=pd.date_range('20200101',periods=12), columns=['A', 'B', 'C'])df 선택head head(n=5)Series 또는 DataFrame의 처음 행부터 n개의 행(상위)을 조회한 결과를 반환합니다. 상위 5개 조회 (DataFrame)1df.head() 상위 3개 조회 (DataFrame)1df.head(3) 상위 5개 조회 (Series)1df['A'].head() 상위 5개 조회 (Series)1df['A'].head(3) tail tail(n=5)Series 또는 DataFrame의 마지막 행부터 n개의 행(하위)을 조회한 결과를 반환합니다. 하위 5개 조회 (DataFrame)1df.tail() 상위 3개 조회 (DataFrame)1df.tail(3) 상위 5개 조회 (Series)1df['A'].tail() 상위 5개 조회 (Series)1df['A'].tail(3) sample sample(n=1, frac=None)Series 또는 DataFrame에서 랜덤하게 행을 조회한 결과를 반환합니다. 특정한 개수를 지정하고 싶은 경우 n 파라미터를 비율을 지정하고 싶은 경우 frac 파라미터를 지정합니다. 1개 행 조회 (DataFrame)1df.sample() 5개 행 조회 (DataFrame)1df.sample(n=5) 30% 행 조회 (DataFrame)1df.sample(frac=0.3) select_dtypes select_dtypes(include=None, exclude=None)DataFrame에서 특정 자료형의 컬럼만 선택하거나 제외한 결과를 반환합니다.정수형 컬럼 선택1df.select_dtypes(include=['int64']) 정수형 컬럼 제외 선택1df.select_dtypes(exclude=['int64']) pop pop(item)item 매개변수로 전달된 컬럼을 반환합니다. 반환된 컬럼은 제거됩니다.pop 함수 적용12df_c = df.pop('C')df_c pop 결과 확인1df query query(expr)조건의 결과가 True인 행을 반환합니다.A컬럼의 값이 B컬럼의 값보다 큰 경우 반환1df.query('A &gt; B') 또한 다음과 같이 사용할 수 있습니다. A컬럼의 값이 B컬럼의 값보다 큰 경우 반환1df[df['A'] &gt; df['B']] 계산 / 기술 통계abs abs()Series 또는 DataFrame의 각 요소의 절댓값 변환된 결과를 반환합니다. 단, 요소가 숫자로 구성된 경우 적용됩니다.음수 변환12df2 = -dfdf2 abs 함수 적용 (DataFrame)1df2.abs() abs 함수 적용 (Series)1df2['A'].abs() corr corr(method=’pearson’, min_periods=1)NA/Null 값을 제외한 컬럼간의 상관 계수를 반환합니다. pearson, kendall, spearman 상관 계수를 적용할 수 있습니다. 기본적으로 pearson 상관계수가 적용되며 변경하고자 하는 경우 method 매개변수에 값을 지정합니다. pearson 또는 spearman 상관 계수를 적용시 최소 관측치의 수를 지정할 수 있습니다. 기본적으로 1개 이상의 관측치가 있는 컬럼을 대상으로 하며 변경하고자하는 경우 min_periods 매개변수에 값을 지정합니다. pearson 상관 계수 적용1df.corr() kendall 상관 계수 적용1df.corr(method='kendall') count count(axis=0, level=None, numeric_only=False)각 열 또는 행의 NA를 제외한 데이터의 개수를 반환합니다. axis 매개변수에 값을 지정하여 열 또는 행을 적용할 수 있습니다. 만약 숫자형(float, int, boolean)만 계산하고 싶은 경우 numeric_only 매개변수에 인수 True를 전달합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열별 데이터의 개수를 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행별 데이터의 개수를 반환합니다. 각 열별 데이터의 개수1df.count(axis=0) 각 행별 데이터의 개수lang:Python1df.count(axis=1) describe describe()NA를 제외한 데이터의 기술 통계량을 반환합니다. 기술 통계량에는 데이터 집합 분포의 중심 경향, 분산 및 모양을 요약한 통계량이 포함됩니다.기술 통계량 계산1df.describe() diff diff(periods=1, axis=0)현재 요소와 이전 요소 간의 차이를 반환합니다. 기본적으로 $$N_i - (N_i-j)$$를 반환합니다. periods 매개변수의 값을 변경하여 이동 간격(j)을 조절할 수 있습니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 차이를 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 차이를 반환합니다. 각 열 별 행의 값 차이 계산1df.diff() 각 행 별 열의 값 차이 계산1df.diff(axis=1) max max(axis=0)각 열 또는 행의 최댓값을 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 최댓값을 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 최댓값을 반환합니다. 각 열 별 최댓값1df.max() 각 행 별 최댓값1df.max(axis=1) mean mean(axis=0)각 열 또는 행의 평균값을 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 평균값을 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 평균값을 반환합니다. 각 열 별 평균값 계산1df.mean() 각 행 별 평균값 계산1df.mean(axis=1) median median(axis=0)각 열 또는 행의 중앙값을 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 중앙값을 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 중앙값을 반환합니다. 각 열 별 중앙값 계산1df.median() 각 행 별 중앙값 계산1df.median(axis=1) min min(axis=0)각 열 또는 행의 최솟값을 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 최솟값을 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 최솟값을 반환합니다. 각 열 별 최솟값 계산1df.min() 각 행 별 최솟값 계산1df.min(axis=1) pct_change pct_change(axis=0, periods=1)현재 요소와 이전 요소 간의 변화율을 반환합니다. 기본적으로 바로 이전 행에서 변경된 백분율을 계산하며 이는 시계열 요소의 변경 비율을 비교하는 데 유용합니다. periods 매개변수의 값을 변경하여 이동 간격을 조절할 수 있습니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 행의 차이를 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 열의 차이를 반환합니다. 각 열 별 행 변화율 계산1df.pct_change() 각 행 별 열 변화율 계산1df.pct_change(axis=1) rank rank()각 열 또는 행의 숫자 데이터 순위를 계산합니다. 기본적으로 동일한 값에는 해당 값 순위의 평균인 순위가 할당됩니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 행의 순위를 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 열의 순위를 반환합니다. 1df.rank() 1df.rank(axis=1) round round(decimals=0)Series 또는 DataFrame의 반올림 결과를 반환합니다.소수점 2자리 표현1pd.DataFrame(np.random.randn(5, 5)).round(2) sum sum(axis=0)각 열 또는 행의 합계를 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 합계를 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 합계를 반환합니다. 각 열 별 합계 계산1df.sum() 각 행 별 합계 계산1df.sum(axis=1) std std(axis=0)각 열 또는 행의 표준 편차값을 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 표준 편차값을 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 표준 편차값을 반환합니다. 각 열 별 표준 편차 계산1df.std() 각 행 별 표준 편차 계산1df.std(axis=1) var var(axis=0)각 열 또는 행의 분산값을 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 분산값을 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 분산값을 반환합니다. 각 열 별 분산 계산1df.var() 각 행 별 분산 계산1df.var(axis=1) nunique nunique(axis=0)각 열 또는 행의 고유값의 개수를 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 고유값의 개수를 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 고유값의 개수를 반환합니다. 각 열 별 고유값 개수 계산1df.nunique() 각 행 별 고유값 개수 계산1df.nunique(axis=1) value_counts value_counts(normalize=False, sort=True, ascending=False)Sereis 또는 DataFrame의 고유값별 데이터의 개수를 반환합니다. DataFrame을 지정하는 경우 모든 열의 값이 동일해야 일치하는것으로 계산하기 때문에 주로 Series를 대상으로 적용합니다. 기본적으로 내림차순 정렬 결과를 반환하며 오름차순 정렬 결과를 적용하고 싶은 경우 ascending 매개변수에 인수 True를 전달합니다. 정규화된 값(비율)을 계산하고 싶은 경우 normalize 매개변수에 인수 True을 전달합니다. 고유값별 데이터 개수 계산12df3 = pd.DataFrame(np.random.randint(0, 3, 25).reshape(5, 5))df3.value_counts() 0 컬럼의 고유값별 데이터 개수 계산1df3[0].value_counts() 0 컬럼의 고유값별 데이터 개수 계산1df3.value_counts([0]) cumsum cumsum(axis=0)각 열 또는 행의 누적 합계를 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 누적 합계를 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 누적 합계를 반환합니다. 각 열 별 누적 합계 계산1df.cumsum() 각 행 별 누적 합계 계산1df.cumsum(axis=1) cumprod cumprod(axis=1)각 열 또는 행의 누적 곱을 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 누적 곱을 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 누적 곱을 반환합니다. 각 열 별 누적 곱 계산1df.cumsum() 각 행 별 누적 곱 계산1df.cumsum(axis=1) idxmax idxmax(axis=0)각 열 또는 행의 최댓값의 인덱스를 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 최댓값의 인덱스를 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 최댓값의 인덱스를 반환합니다. 각 열 별 최댓값 인덱스1df.idxmax() 각 행 별 최댓값 인덱스1df.idxmax(axis=1) idxmin idxmin(axis=0)각 열 또는 행의 최솟값의 인덱스를 반환합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 각 열 별 최솟값의 인덱스를 반환합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 각 행 별 최솟값의 인덱스를 반환합니다. 각 열 별 최솟값 인덱스1df.idxmin() 각 행 별 최솟값 인덱스1df.idxmin(axis=1) 삭제 및 색인drop drop(labels=None, axis=0)지정된 레이블을 행 또는 열에서 제거합니다. 삭제하고자 하는 행 또는 열이 1개인 경우 행 인덱스 또는 열 인덱스를 문자열로 여러개인 경우 문자열 리스트로 지정합니다. axis이 0으로 지정된 경우 행을 기준으로 하기 때문에 해당 행을 삭제 합니다.axis이 1로 지정된 경우 열을 기준으로 하기 때문에 해당 열을 삭제 합니다. 단일 행 삭제1df.drop(df.index[0], axis=0) 복수 행 삭제1df.drop(df.index[:3], axis=0) 단일 열 삭제1df.drop('A', axis=1) 복수 열 삭제1df.drop(['A','C'], axis=1) drop_duplicates drop_duplicates(subset=None, keep=’first’)중복된 행을 제거한 결과를 반환합니다. 모든 열이 동일한 경우 제거되며 특정 열만 고려하도록 subset 매개변수에 인수를 전달할 수 있습니다. keep 매개변수에 인수 ‘first’가 전달되면 첫번째 데이터를 ‘last’가 전달되면 마지막 데이터를 남깁니다. 중복된 행 제거1df3.drop_duplicates() 중복된 행 제거 (기준 적용)lang:Python1df3.drop_duplicates(0) rename rename()행 인덱스 또는 열 인덱스를 변경할 수 있습니다. 변경하고자 하는 대상을 Key로 변경하고자 하는 값을 Value로 하는 Dict 형태로 값을 전달합니다.행 인덱스 변경1df3.rename(index={0:'A'}) 열 인덱스 변경1df3.rename(columns={0:'A'}) reset_index reset_index(drop=False)행 인덱스를 재설정한 결과를 반환합니다. 기존의 인덱스는 열로 추가되며 삭제하고 싶은 경우 drop 매개 변수에 인수 True를 전달합니다.인덱스 재설정1df.reset_index() set_index set_index()지정된 열을 인덱스로 지정한 결과를 반환합니다.12df['D'] = ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B']df.set_index('D')","link":"/2021/01/21/PANDAS-FUNCTION/"},{"title":"Grouping","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 Series와 DataFrame 객체를 이용하여 데이터를 분석 및 조작할 수 있습니다. 본 글에서는 Grouping에 대해 알아봅니다. Pandas에서의 Grouping은 다음과 같은 단계를 거칩니다. Splitting 일부 기준에 따라 데이터를 그룹으로 분할 Applying 각 그룹에 독립적으로 기능 적용 Combining 결과를 데이터 구조로 결합 즉, 특정 컬럼의 값을 기준으로 데이터 프레임을 분할하여 분할된 그룹을 집계 후 결합된 데이터 프레임이 반환되는 과정입니다. Grouping의 경우 다양한 적용 방법과 그로 인한 다양한 실행 결과가 나옵니다. 따라서 본인의 스타일에 맞는 방법을 사용하는 것을 권장하며 본 예시에서는 가장 쉽고 단순한 방법을 사용합니다. 단 해당 방법을 사용하는 경우 Hierarchical Index (MultiIndex)로 인해 결과를 파악하는게 더 어려울수 있음을 유의하시길바랍니다. Pandas에서 groupby 함수를 이용하여 Grouping을 적용할 수 있습니다. groupby groupby(by=None) by 파라미터에 분할하고자 하는 기준 컬럼명을 전달합니다. 이후 분할된 데이터 프레임에 통계 또는 연산 함수를 이용하여 데이터를 요약하거나 집계를 적용할 수 있습니다. 가상 데이터 생성2019년 01월 ~ 2020년 12월까지의 동부 지역과 서부 지역의 월간 상품 판매 데이터를 생성합니다. 데이터 생성123456789101112131415161718import numpy as npimport pandas as pdnp.random.seed(2021)date = pd.date_range('20190101','20201231', freq='MS')df_a = pd.DataFrame({ '판매량':np.random.randint(1000, 2000, 24), '지역': '동부', '연도': date.year, '월': date.month})df_b = pd.DataFrame({ '판매량':np.random.randint(1000, 2000, 24), '지역': '서부', '연도': date.year, '월': date.month})df = pd.concat([df_a, df_b]).reset_index(drop=True)df 사용 방법지역별 판매량 합계를 보고 싶은 경우 다음과 같이 groupby를 적용해볼수가있습니다. 지역별 판매량 합계1df.groupby('지역').sum() 집계 대상은 연산 가능한 모든 컬럼이 지정되며 집계 방법(함수)는 합계(sum), 평균(mean), 중앙값(median), 최대(max), 최소(min), 기술 통계(describe)등 다양한 함수를 적용할 수 있습니다. 데이터가 많은 경우 불필요한 컬럼까지 집계하는 경우 실행 시간이 오래걸릴 수 있습니다. 이를 위해서는 집계 대상을 지정하여 해당 대상만 집계하도록 합니다. 이를 위해서는 사전 표기법을 이용하여 집계하고자 하는 컬럼을 선택할 수 있습니다. 지역별 판매량 합계1df.groupby('지역')['판매량'].sum() DataFrame에 문자열을 이용하여 컬럼을 선택하는 경우 Series가 선택됩니다. 따라서 집계 결과도 Series로 반환됩니다. 만약 DataFrame으로 반환된 결과를 보고싶은 경우 문자열 리스트로 전달합니다. 지역별 판매량 합계1df.groupby('지역')[['판매량']].sum() 반환된 결과의 컬럼명이 선택한 컬럼명이 된것을 유심히 확인하며 이번에는 집계 방법을 변경해보도록하겠습니다. 단일 함수를 적용할 수 있지만 여러 집계 함수를 적용하고자 하는 경우 agg 함수를 이용할 수 있습니다. 지역별 판매량 합계1df.groupby('지역')['판매량'].agg('sum') 집계 대상과 집계 방법이 모두 단일로 선택되어 응답 결과가 Series가 반환되는것을 확인할수있습니다. 지역별 판매량 합계1df.groupby('지역')['판매량'].agg(['sum']) 이때 agg함수의 집계 함수를 문자열 리스트로 지정하는 경우 DataFrame이 반환되며 컬럼명이 집계 함수인것을 확인할 수 있습니다. 집계 대상과 집계 함수를 모두 문자열 리스트로 지정하는 경우 MultiIndex가 적용됨을 확인할 수 있습니다. 지역별 판매량 합계1df.groupby('지역')[['판매량']].agg(['sum']) 컬럼명 확인1df.groupby('지역')[['판매량']].agg(['sum']).columns 이와 같이 적용 코드에 따른 결과를 알아야 하는 이유는 컬럼을 선택하거나 정렬하고자 하는 경우 MultiIndex로 지정해야하기 때문입니다. 지역별 판매량 합계 (내림차순 정렬)1df.groupby('지역')[['판매량']].agg(['sum']).sort_values(('판매량', 'sum'), ascending=False) 분할 기준도 여러 컬럼을 문자열 리스트로 전달하여 데이터를 분할할 수 있습니다. 이때 분할 기준은 응답 결과에 영향을 주지 않는다는것을 확인할수있습니다. 지역별 연도별 판매량 합계1df.groupby(['지역', '연도'])['판매량'].agg('sum') 결론만약 초보자인 경우 다음과 같은 형식을 사용하는 것을 권장합니다. 그룹 집계1234group_col = [] # 분할 컬럼target_col = [] # 집계 컬럼agg_func = [] # 집계 함수df.groupby(group_col)[target_col].agg(agg_func) 해당 코드를 사용하는 경우 분할 컬럼, 집계 컬럼, 집계 함수만 지정하면 됩니다. 단 해당 방법을 사용하는 경우 MultiIndex가 발생하기때문에 집계 결과에 columns 속성을 이용하여 컬럼명을 확인 후 작업하는것을 권장합니다.","link":"/2021/01/24/PANDAS-GROUPING/"},{"title":"Missing Data","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 Series와 DataFrame 객체를 이용하여 데이터를 분석 및 조작할 수 있습니다. 본 글에서는 결측치를 다루는 방법에 대해 알아봅니다. Pandas에서 결측치는 np.nan을 사용합니다. 또한 sum, mean과 같은 연산에서 포함되지 않는다는 특징이 있습니다. 데이터 분석 및 머신러닝 과정에서 결측치를 찾고 대체하거나 제거하는것은 매우 중요합니다. 결측치가 포함되어 있는 경우 학습이 정상적으로 되지 않거나 데이터의 값이 변형될 수 있습니다. 결측치 비교Pandas의 isna 또는 isnull 함수와 notna 또는 notnull함수를 사용하여 결측치를 비교할 수 있습니다. 값과 결측치를 비교하는 함수가 존재하는 이유는 비교 연산자를 이용하여 값을 비교할 수 없기 때문입니다. 결측치 비교결측치 비교12print(np.nan == None)print(np.nan == pd.Series(np.nan).iloc[0]) 결측치 비교1pd.Series(np.nan).isna() 결측치 비교1pd.Series(np.nan).notna() 결측치 조회대량의 데이터 프레임에서 각 행 또는 열의 결측치를 찾기 위해서는 sum 함수를 이용합니다. 데이터 프레임 생성임의의 숫자를 생성하여 0 이하인 경우 결측치인 데이터 프레임을 생성합니다. 결측치 데이터 프레임 생성123df = pd.DataFrame(np.random.randn(1000, 10))df = df[df&gt;0]df 결측치 비교생성된 데이터프레임에 결측치 비교 함수를 이용하여 해당 데이터가 결측치 인지 아닌지를 알려주는 Boolean DataFrame을 생성합니다. 결측치 데이터 프레임 생성1df.isna() 해당 Boolean DataFrame에 sum 함수를 적용하면 각 행 또는 각 열의 결측치의 수를 계산할 수있습니다. True는 1, Fasle는 0으로 계산됩니다. 각 열별 결측치 수 계산1df.isna().sum(axis=0) 각 행별 결측치 수 계산1df.isna().sum(axis=1) 결측치 비율 계산결측치 계산 방법을 이용하여 각 행 또는 열의 NA 비율을 계산할 수 있습니다. 각 행 별 결측치 비율 계산12row_na_rate = df.isna().sum(axis=1) / df.shape[1]row_na_rate 각 열 별 결측치 비율 계산12col_na_rate = df.isna().sum(axis=0) / df.shape[0]col_na_rate 결측치 삭제결측치를 대처하는 가장 좋은 방법은 결측치를 삭제하는 방법입니다. Pandas에서는 dropna함수를 이용하여 결측치를 제거할 수 있습니다. dropna() dropna(axis=0, how=’any’, thresh=None)dropna 함수를 적용하면 기본적으로 결측치가 1개라도 존재하는 행을 제거합니다. 만약 열을 제거하고 싶은 경우 axis 매개변수에 인수 1을 전달합니다. 결측치 제거1df.dropna() dropna 함수를 이용하여 결측치를 제거하는 경우 많은 데이터가 삭제될 수 있습니다. 이를 해결하기 위해서는 how 매개변수에 인수 all을 전달하여 모든 행 또는 모든 열이 결측치인 경우 제거할 수 있습니다. 결측치 제거1df.dropna(how='all') 이번에는 너무 많은 행 또는 열이 남아있는 문제가 발생합니다. 따라서 적절한 임계값을 지정하여 결측치를 제거하는 방법에 대해 생각해볼수있습니다. 이를 위해서는 thresh 매개변수에 기준 값을 전달합니다. 해당 기준 값은 NA가 아닌 데이터의 수로 만약 10개의 데이터 중 결측치가 3개가 존재하는 경우 thresh를 7로 지정하면 해당 행 또는 열은 삭제 안하지만 8로 지정하면 해당 행 또는 열은 삭제됩니다. 또한 thresh 매개 변수에는 정수값만 전달될 수 있습니다. 결측치 제거12d = pd.DataFrame([1, 2, 3, 4, 5, 6, 7, np.nan, np.nan, np.nan])d.dropna(thresh=7, axis=1) 결측치 제거12d = pd.DataFrame([1, 2, 3, 4, 5, 6, 7, np.nan, np.nan, np.nan])d.dropna(thresh=8, axis=1) 따라서 다음과 같이 NA의 비율을 계산하여 thresh 값을 선정할 수 있습니다. 결측치 제거 (열)123rate = 0.2 # NA 비율threshold = df.shape[0] * (1 - rate)df.dropna(thresh=threshold, axis=1) 결측치 제거 (행)123rate = 0.2 # NA 비율threshold = df.shape[1] * (1 - rate)df.dropna(thresh=threshold, axis=0) 결측치 대체결측치를 삭제하는것이 가장 좋은 방법이지만 데이터 손실이 너무 크기 때문에 현실적으로 결측치를 삭제하는데는 많은 어려움이 있습니다. 따라서 결측치를 적절한 값으로 대체하는 방법을 사용합니다. 이를 보간법(interpolation)이라고 합니다. 결측치 대체에는 다양한 방법이 있는데 대표적으로 평균(mean) 또는 중앙값(median)을 적용할 수 있습니다. 해당 방법을 이용하면 빠르게 결측치를 대체할 수 있지만 원본 데이터에 영향이 있어 통계 분석시 방해가 될 수 있습니다. 이를 판단하기 위해서는 해당 데이터에 대한 이해 즉 도메인 지식이 중요합니다. Pandas에서는 fillna함수를 이용하여 결측치를 대체할 수 있습니다. fillna() fillna(value=None, method=None) 단일 값으로 결측치를 대체하고 싶은 경우 해당 값을 value 매개변수에 전달합니다. 결측치 대체1df.fillna(0) 만약 각 컬럼별로 특정 값을 대체하고 싶으면 대체하고자 하는 축의 인덱스 Key로 가지는 Dict 또는 Index로 가지는 Series를 이용합니다. 각 컬럼별 평균 또는 중앙값을 이용하여 대체하려는 경우 mean 또는 median 함수를 이용하면 쉽게 대체할 수 있습니다. 결측치 대체1df.fillna(df.mean()) 만약 데이터가 시계열 데이터인 경우 결측치 발생 이전 또는 이후의 값을 대체하는게 좋은 방법일 수도 있습니다. 예를들어 2초전 데이터가 36.1도 1초전 데이터가 36.1도 1초후 데이터가 36.1도인 데이터가 존재하는 경우 전체의 평균을 이용하는것보다 조금 더 정확할 수 있습니다. 만약 결측치가 발생한 이전 정상값으로 대체하고 싶은 경우 method 매개변수에 인수 ffill(forward fill)을 전달합니다. 결측치 대체12series = pd.Series([36.1, 36.1, np.nan, 36.2])series.fillna(method='ffill') 결측치가 발생한 이후 정상값으로 대체하고 싶은 경우 method 매개변수에 인수 bfill(backword fill)을 전달합니다. 결측치 대체12series = pd.Series([36.1, 36.1, np.nan, 36.2])series.fillna(method=bfill')","link":"/2021/01/23/PANDAS-MISSING-DATA/"},{"title":"Pivoting","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 Series와 DataFrame 객체를 이용하여 데이터를 분석 및 조작할 수 있습니다. 본 글에서는 Pivoting에 대해 알아봅니다. PivotPivot은 선회 축을 의미합니다. Pivoting은 축을 중심으로 데이터나 결과값을 유동적으로 이동하고 변경하면서 많은 양의 데이터를 다양하게 분석할 수 있는 방법이라고 할 수 있습니다. 기존의 축 대신 새로운 축을 사용하여 값을 배치(재배치)하여 통계적인 값을 보기 위한 테이블을 만드는 과정 엑셀의 대표적인 피벗테이블과 동일하게 Pandas에서도 pivot 함수와 pivot_table 함수가 존재합니다. pivot 함수의 경우 단순 데이터를 재배치만 하고 집계 기능은 존재하지 않아 pivot_table 함수를 주로 이용합니다. pivot_table pivot_table(index=None, columns=None, values=None, aggfunc=’mean’, fill_value=None, margins=Fasle, margins_name=’All’) 사용 방법groupby 사용법과는 다르게 모두 pivot_table은 모든 설정을 파라미터로 지정합니다. 행 인덱스와 열 인덱스로 지정할 컬럼을 각각 index와 columns 매개 변수에 집계 대상을 values 컬럼으로 지정합니다. 기본적으로 집계 방법은 평균이 지정되며 다른 집계 방법을 사용하고자하는 경우 aggfunc 매개변수에 원하는 집계 함수를 전달합니다. 재배치 과정에서 데이터가 존재하지 않는 경우 NA값이 발생하며 fill_value 매개변수를 통해 NA를 대체할 값을 지정할 수 있습니다. 또한 각 행과 열의 평균(합)을 계산하고 싶은 경우 margins 매개변수에 True 값을 전달합니다. 가상 데이터 생성2019년 01월 ~ 2020년 12월까지의 동부 지역과 서부 지역의 월간 상품 판매 데이터를 생성합니다. 데이터 생성123456789101112131415161718import numpy as npimport pandas as pdnp.random.seed(2021)date = pd.date_range('20190101','20201231', freq='MS')df_a = pd.DataFrame({ '판매량':np.random.randint(1000, 2000, 24), '지역': '동부', '연도': date.year, '월': date.month})df_b = pd.DataFrame({ '판매량':np.random.randint(1000, 2000, 24), '지역': '서부', '연도': date.year, '월': date.month})df = pd.concat([df_a, df_b]).reset_index(drop=True)df 데이터 변형연도별 지역 판매량 합계를 보고 싶은 경우 다음과 같이 groupby를 적용해볼수가있습니다. 지역별 연 판매량1df.groupby(['지역','연도'])['판매량'].agg(['sum']) 해당 결과를 이용하여 시각화를 하는 경우 Index가 X축으로 설정되기 때문에 비교가 어렵다는 단점이 존재합니다. 지역별 연 판매량 시각화1df.groupby(['지역','연도'])['판매량'].agg(['sum']).plot(kind='bar') 이런 경우 pivot_table을 이용하면 쉽게 두 지역의 판매량을 비교하는 그래프를 만들어볼수있습니다. 지역별 연 판매량1df.pivot_table(index='연도', columns='지역', values='판매량', aggfunc='sum') 지역별 연 판매량 시각화1df.pivot_table(index='연도', columns='지역', values='판매량', aggfunc='sum').plot(kind='bar') 또한 margins과 margins_name 매개변수를 이용하여 행과 열의 합계를 추가할 수 있습니다. 지역별 연 판매량1df.pivot_table(index='연도', columns='지역', values='판매량', aggfunc='sum', margins=True, margins_name='합계') 즉 pivot_table 함수는 groupby는 다르게 열 인덱스도 지정가능하여 유연한 형태의 테이블을 생성할 수 있습니다. 지역별 월간 판매량1ddf.pivot_table(index='연도', columns=['지역','월'], values='판매량', aggfunc='sum', margins=True, margins_name='합계') 지역별 월간 판매량1234fig, axes = plt.subplots(nrows=1, ncols=2)df.pivot_table(index='월', columns=['지역','연도'], values='판매량', aggfunc='sum')['동부'].plot(figsize=(20, 5), title='동부지역 판매량', ax=axes[0])df.pivot_table(index='월', columns=['지역','연도'], values='판매량', aggfunc='sum')['서부'].plot(title='서부지역 판매량',ax=axes[1])plt.show()","link":"/2021/01/25/PANDAS-PIVOTING/"},{"title":"Merge","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 Series와 DataFrame 객체를 이용하여 데이터를 분석 및 조작할 수 있습니다. 본 글에서는 다수의 Series 또는 DataFrame을 합치는 방법에 대해 알아봅니다. Pandas의 concat 함수와 merge함수를 사용하여 다수의 Series 또는 DataFrame을 합칠수 있습니다. Concatenate concat([], axis=0, join=’outer’, ignore_index=Fasle)concat 함수는 다수의 Series 또는 DataFrame를 하나로 합치기 위해 사용하는 함수입니다. 합치고자 하는 Series 또는 DataFrame을 하나의 리스트로 묶어서 사용합니다. 기본적으로 행과 행 결합(상하 결합)을 하며 열과 열 결합(좌우 결합)을 하고 싶은 경우 axis 매개변수에 인수 1을 전달합니다. 동일한 구조(행/열)을 가지는 경우 사용할 수 있습니다. 만약 합치려고 하는 Series 또는 DataFrame이 서로 다른 형태인 경우 NA로 채워진 데이터가 발생합니다. 이를 해결하기 위해서는 join 매개변수에 인수 ‘inner’를 전달합니다. outer join이 기본적으로 적용되며 inner join을 사용시 중복되는 구조만 합쳐짐 또한 단순히 데이터를 연결하기 때문에 index가 중복될 수 있습니다. 이를 해결하기 위해서는 ignore_index 매개변수에 인수 True를 전달합니다. Series 결합123s1 = pd.Series(['a', 'b'])s2 = pd.Series(['c', 'd'])pd.concat([s1, s2]) Series 결합 (인덱스 무시)1pd.concat([s1, s2], ignore_index=True) DataFrame 결합123df1 = pd.DataFrame([['a', 1], ['b', 2]], columns=['letter', 'number'])df2 = pd.DataFrame([['c', 3], ['d', 4]], columns=['letter', 'number'])pd.concat([df1, df2]) DataFrame 결합 (NA 발생)12df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']], columns=['letter', 'number', 'animal'])pd.concat([df1, df2, df3]) DataFrame 결합 (inner join)12df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']], columns=['letter', 'number', 'animal'])pd.concat([df1, df2, df3], join='inner') Merge merge(left, right, how=’inner’, on=’label’)merge 함수는 두개의 DataFrame을 결합하기 위해서 사용합니다. 데이터 베이스의 Join 연산과 동일한 기능을 수행할 수 있습니다. 결합 방법은 inner, outer, right, left, cross join을 지원합니다. 단, cross join의 경우 1.2.0 이상 버전에서만 사용 가능합니다. merge는 concat과는 다르게 공통 값을 가지는 컬럼을 이용하여 결합하는 방법입니다. 따라서 동일한 값을 가지면서 서로 다른 데이터를 합치는 경우 사용합니다. Inner Join123df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})pd.merge(df1, df2, on='a', how='inner') Outer Join1pd.merge(df1, df2, on='a', how='outer') Left Join1pd.merge(df1, df2, on='a', how='left') Right Join1pd.merge(df1, df2, on='a', how='right')","link":"/2021/01/22/PANDAS-MERGE/"},{"title":"Plot","text":"Pandas는 Python 프로그래밍 언어를 기반으로 구축된 빠르고 강력하며 유연하고 사용하기 쉬운 오픈 소스 데이터 분석 및 조작 도구입니다. Pandas에서는 Series와 DataFrame 객체를 이용하여 데이터를 분석 및 조작할 수 있습니다. 본 글에서는 Pandas에서 시각화 방법에 대해 알아봅니다. Python에서 데이터를 시각화 하기 위해서 가장 많이 사용되는 모듈은 matplotlib 입니다. Pandas는 ecosystem으로 matplotlib를 포함하고 있어 간단하게 데이터를 시각화할 수 있습니다. Pandas에서 데이터를 시각화 하기 위해서는 plot함수를 이용할 수 있습니다. 또한 Series 또는 DataFrame의 Index가 X축 Value가 Y축으로 지정됩니다. DataFrame의 경우 각 컬럼이 하나의 그래프가 됩니다. Line Plotplot 함수를 사용하면 기본적으로 Line Plot으로 시각화 합니다. Line Plot12ts = pd.Series(np.random.randn(366), index=pd.date_range(&quot;1/1/2020&quot;, periods=366)).cumsum()ts.plot() 만약 DataFrame을 시각화 하는 경우 모든 컬럼이 같은 Y축을 가지는 시각화 결과물이 생성됩니다. Line Plot12ts_df = pd.DataFrame(np.random.randn(366, 4), index=ts.index, columns=list(&quot;ABCD&quot;)).cumsum()ts_df.plot() Line Plot은 일련의 값을 시각화하는 간단한 방법으로 시간별 추세 또는 미래 값을 예측하려는 경우 사용할 수 있습니다. Area Plotplot 함수에 kind 매개변수에 원하는 그래프 유형을 전달하면 해당 그래프로 시각화할 수 있습니다. Area Plot으로 시각화 하기 위해서는 데이터가 전부 양수여야합니다. Line Plot1ts_df.abs.plot(kind='area') 영역 차트는 라인과 축 사이의 공간이 색상으로 채워진 라인 차트로 시간별 누계를 표현하고 비교할 수 있습니다. Area Plot1df.pivot_table(index=['연도', '월'], columns='지역', values='판매량').plot(kind='area') Bar PlotBar Plot으로 시각화 하기 위해서는 kind 매개변수에 ‘bar’ 또는 ‘barh’를 전달합니다. Bar Plot1ts_df.plot(kind='bar') 여러 범주의 데이터를 비교하기 위해 주로 사용합니다. Bar Plot1df.groupby('지역')['판매량'].sum().plot(kind='bar') X축과 Y축을 반전하고 싶은 경우 barh를 전달합니다. Horizontal Bar Plot1df.groupby('지역')['판매량'].sum().plot(kind='barh') Box PlotBox Plot으로 시각화 하기 위해서는 kind 매개변수에 ‘box’를 전달합니다. Box Plot1ts_df.plot(kind='box') Box Plot은 주로 이상치를 찾기 위해 많이 사용되는 통계 그래프입니다. Pandas에서는 1.5 IQR Rule을 이용하여 데이터를 시각화 합니다. Box Plot1df['판매량'].plot(kind='box') Density PlotDensity Plot으로 시각화 하기 위해서는 kind 매개변수에 ‘kde’ 또는 ‘density’를 전달합니다. Density Plot1ts_df.plot(kind='kde') 통계에서 커널 밀도 추정(KDE)은 랜덤 변수의 확률 밀도 함수(PDF)를 추정하는 비모수 방법으로 가우스 커널을 사용합니다. 이러한 시각화는 주로 데이터의 분포를 보기위해 많이 사용합니다. Density Plot1df['판매량'].plot(kind='kde') Hexagonal Binning PlotHexagonal Binning Plot 으로 시각화 하기 위해서는 kind 매개변수에 ‘hexbin’를 전달합니다. Hexagonal Binning Plot1ts_df.plot(x='A', y='B', kind='hexbin') Hexagonal Binning Plot은 대량의 데이터의 분포를 시각화 하기 위해서 주로 사용됩니다. 밀도가 높은 지역은 진한 색상으로 밀도가 낮은 지역은 낮은 색상으로 표현되며 산점도에서 데이터가 중첩되는 문제를 해결할 수 있습니다. X축과 Y축이 위도와 경도로 지정되는 경우 마치 지도를 시각화 한것과 비슷한 효과를 얻을 수 있습니다. Histogram PlotHistogram Plot으로 시각화 하기 위해서는 kind 매개변수에 ‘hist’ 를 전달합니다. Density Plot1ts_df.plot(kind='hist') Histogram은 표로 되어 있는 도수 분포를 정보 그림으로 나타낸 것으로 이러한 시각화는 주로 데이터의 분포를 보기위해 많이 사용합니다. Density Plot1df['판매량'].plot(kind='hist') 분할 구간을 변경하기위해서는 bins 매개변수에 값을 지정할수있습니다. Density Plot1df['판매량'].plot(kind='hist', bins=30) Pie PlotPie Plot으로 시각화 하기 위해서는 kind 매개변수에 ‘pie’ 를 전달합니다. Pie Plot1ts_df.sum().plot(kind='pie') Pie Plot은 범주가 차지하는 비율을 비교하기 위해서 사용합니다. Pie Plot1df.groupby('지역')['판매량'].sum().plot(kind='pie') 또한 Pie Plot의 경우 autopct 매개변수에 포맷팅 형식을 지정하여 각 범주의 비율을 시각화할수 있습니다. Pie Plot1df.groupby('지역')['판매량'].sum().plot(kind='pie', autopct='%.2f%%') Scatter PlotScatter Plot으로 시각화 하기 위해서는 kind 매개변수에 ‘scatter’을 전달합니다. Pie Plot1ts_df.plot(x='A', y='B', kind='scatter') Scatter Plot은 직교 좌표계를 이용하여 X축과 Y축의 관계를 나타내기 위한 방법으로 상관 관계를 보기위해 많이 사용됩니다. 기타 매개 변수figsizefigsize 매개변수를 이용하여 Figure 객체의 크기를 지정할 수 있습니다. (단위는 인치입니다.) Density Plot1ts_df.plot(kind='kde', figsize=(10, 10)) cc 매개 변수를 이용하여 그래프의 색상을 지정할 수 있습니다. 색상은 hex color도 지정 가능합니다. Scatter Plot1ts_df.plot(x='A', y='B', kind='scatter', s=100, c='red') subplots, layoutsubplots 매개변수와 layout 매개변수를 이용하여 그래프 시각화 결과를 분할할 수 있습니다. Density Plot1ts_df.plot(kind='kde', figsize=(10, 10), subplots=True, layout=(2, 2)) sScatter Plot의 경우 s 매개변수를 이용하여 크기를 조절할 수 있습니다. Scatter Plot1ts_df.plot(x='A', y='B', kind='scatter', s=100)","link":"/2021/01/26/PANDAS-PLOT/"}],"tags":[{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Pandas","slug":"Pandas","link":"/tags/Pandas/"},{"name":"COVID-19","slug":"COVID-19","link":"/tags/COVID-19/"},{"name":"Crawling","slug":"Crawling","link":"/tags/Crawling/"},{"name":"Setting","slug":"Setting","link":"/tags/Setting/"},{"name":"실거래가","slug":"실거래가","link":"/tags/%EC%8B%A4%EA%B1%B0%EB%9E%98%EA%B0%80/"},{"name":"오픈API","slug":"오픈API","link":"/tags/%EC%98%A4%ED%94%88API/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Scikit-Learn","slug":"Scikit-Learn","link":"/tags/Scikit-Learn/"},{"name":"Series","slug":"Series","link":"/tags/Series/"},{"name":"DataFrame","slug":"DataFrame","link":"/tags/DataFrame/"},{"name":"CSV","slug":"CSV","link":"/tags/CSV/"},{"name":"FILE I&#x2F;O","slug":"FILE-I-O","link":"/tags/FILE-I-O/"},{"name":"EXCEL","slug":"EXCEL","link":"/tags/EXCEL/"},{"name":"Apple Silicon","slug":"Apple-Silicon","link":"/tags/Apple-Silicon/"},{"name":"Jupyter","slug":"Jupyter","link":"/tags/Jupyter/"},{"name":"TensorFlow Certificate","slug":"TensorFlow-Certificate","link":"/tags/TensorFlow-Certificate/"},{"name":"법정동 코드","slug":"법정동-코드","link":"/tags/%EB%B2%95%EC%A0%95%EB%8F%99-%EC%BD%94%EB%93%9C/"}],"categories":[{"name":"공공 데이터","slug":"공공-데이터","link":"/categories/%EA%B3%B5%EA%B3%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0/"},{"name":"코로나 19","slug":"공공-데이터/코로나-19","link":"/categories/%EA%B3%B5%EA%B3%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0/%EC%BD%94%EB%A1%9C%EB%82%98-19/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Pandas","slug":"Python/Pandas","link":"/categories/Python/Pandas/"},{"name":"환경 설정","slug":"환경-설정","link":"/categories/%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/"},{"name":"실거래가","slug":"공공-데이터/실거래가","link":"/categories/%EA%B3%B5%EA%B3%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0/%EC%8B%A4%EA%B1%B0%EB%9E%98%EA%B0%80/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Apple Silicon","slug":"환경-설정/Apple-Silicon","link":"/categories/%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/Apple-Silicon/"},{"name":"Python","slug":"환경-설정/Python","link":"/categories/%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/Python/"},{"name":"Tableau","slug":"환경-설정/Tableau","link":"/categories/%ED%99%98%EA%B2%BD-%EC%84%A4%EC%A0%95/Tableau/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/categories/Deep-Learning/"},{"name":"Scikit-Learn","slug":"Machine-Learning/Scikit-Learn","link":"/categories/Machine-Learning/Scikit-Learn/"},{"name":"TensorFlow","slug":"Deep-Learning/TensorFlow","link":"/categories/Deep-Learning/TensorFlow/"}]}